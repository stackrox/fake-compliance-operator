{
  "apiVersion": "v1",
  "items": [
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Ensure that anonymous requests to the API Server are authorized\nBy default, anonymous access to the OpenShift API is enabled, but at the same time, all requests must be authorized. If no authentication mechanism is used, the request is assigned the system:anonymous virtual user and the system:unauthenticated virtual group. This allows the authorization layer to determine which requests, if any, is an anonymous user authorized to make. To verify the authorization rules for anonymous requests run the following:\n\n$ oc describe clusterrolebindings\n\nand inspect the bindings of the system:anonymous virtual user and the system:unauthenticated virtual group. To test that an anonymous request is authorized to access the readyz endpoint, run:\n\n$ oc get --as=\"system:anonymous\" --raw='/readyz?verbose'\n\nIn contrast, a request to list all projects should not be authorized:\n\n$ oc get --as=\"system:anonymous\" projects",
      "id": "xccdf_org.ssgproject.content_rule_api_server_anonymous_auth",
      "instructions": "Run the following command to view the authorization rules for anonymous requests:\n$ oc describe clusterrolebindings\nMake sure that there exists at least one clusterrolebinding that binds\neither the system:unauthenticated group or the system:anonymous\nuser.\nTo test that an anonymous request is authorized to access the readyz\nendpoint, run:\n$ oc get --as=\"system:anonymous\" --raw='/readyz?verbose'\nIn contrast, a request to list all projects should not be authorized:\n$ oc get --as=\"system:anonymous\" projects",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "api-server-anonymous-auth"
        },
        "creationTimestamp": "2024-03-05T15:10:33Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-bsi",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-bsi-api-server-anonymous-auth",
        "namespace": "openshift-compliance",
        "resourceVersion": "82589",
        "uid": "bb8b941e-5bc8-4b97-ad16-22061d58eff3"
      },
      "rationale": "When enabled, requests that are not rejected by other configured authentication methods are treated as anonymous requests. These requests are then served by the API server. If you are using RBAC authorization, it is generally considered reasonable to allow anonymous access to the API Server for health checks and discovery purposes, and hence this recommendation is not scored. However, you should consider whether anonymous discovery is an acceptable risk for your purposes.",
      "severity": "medium",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Ensure that the kubeadmin secret has been removed\nThe kubeadmin user is meant to be a temporary user used for bootstrapping purposes. It is preferable to assign system administrators whose users are backed by an Identity Provider.\n\nMake sure to remove the user as described in the documentation ( https://docs.openshift.com/container-platform/latest/authentication/remove-kubeadmin.html )",
      "id": "xccdf_org.ssgproject.content_rule_kubeadmin_removed",
      "instructions": "To verify that the kubeadmin secret has been deleted, make sure\nthat oc get secrets kubeadmin -n kube-system\nreturns a NotFound error.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "kubeadmin-removed"
        },
        "creationTimestamp": "2024-03-05T15:10:33Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "FAIL",
          "compliance.openshift.io/scan-name": "ocp4-bsi",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-bsi-kubeadmin-removed",
        "namespace": "openshift-compliance",
        "resourceVersion": "82590",
        "uid": "8e5fbf45-ebfe-4564-b4fb-4ce71c5ab071"
      },
      "rationale": "The kubeadmin user has an auto-generated password and a self-signed certificate, and has effectively\n\ncluster-admin\n\npermissions; therefore, it's considered a security liability.",
      "severity": "medium",
      "status": "FAIL"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Check configured allowed registries for import uses secure protocol\nThe configuration allowedRegistriesForImport limits the container image registries from which normal users may import images. This is a list of the registries that can be trusted to contain valid images and the image location configured is assumed to be secured unless configured otherwise. It is important to allow only secure registries to avoid man in the middle attacks, as the insecure image import request can be impersonated and could lead to fetching malicious content. List all the allowed repositories for import configured with insecure set to true using the following command:\n\noc get image.config.openshift.io/cluster -o json | jq '.spec | (.allowedRegistriesForImport[])? | select(.insecure==true)'\n\nRemove or edit the listed registries having insecure set by using the command:\n\noc edit image.config.openshift.io/cluster\n\nFor more information, follow the relevant documentation ( https://docs.openshift.com/container-platform/latest/openshift_images/image-configuration.html ).",
      "id": "xccdf_org.ssgproject.content_rule_ocp_insecure_allowed_registries_for_import",
      "instructions": "To check for the configured allowedRegistriesForImport with insecure option use below command:\n$ oc get image.config.openshift.io/cluster -ojsonpath='{.spec.allowedRegistriesForImport}'\nThe output lists the configured allowedRegistriesForImport and the insecure option used.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "ocp-insecure-allowed-registries-for-import"
        },
        "creationTimestamp": "2024-03-05T15:10:33Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-bsi",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-bsi-ocp-insecure-allowed-registries-for-import",
        "namespace": "openshift-compliance",
        "resourceVersion": "82591",
        "uid": "47867f77-ce58-46a0-a14b-040ff5851ece"
      },
      "rationale": "Configured list of allowed registries for import should be from the secure source.",
      "severity": "medium",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Check if any insecure registry sources is configured\nThe configuration registrySources.insecureRegistries determines the insecure registries that the OpenShift container runtime can access for builds and pods. This configuration setting is for accessing the configured registries without TLS validation which could lead to security breaches and should be avoided. Remove any insecureRegistries configured using the following command:\n\noc patch image.config.openshift.io cluster --type=json -p \"[{'op': 'remove', 'path': '/spec/registrySources/insecureRegistries'}]\"\n\nFor more information, follow the relevant documentation ( https://docs.openshift.com/container-platform/latest/openshift_images/image-configuration.html ).",
      "id": "xccdf_org.ssgproject.content_rule_ocp_insecure_registries",
      "instructions": "To check for the configured insecure registry sources use below command:\n$ oc get image.config.openshift.io/cluster -ojsonpath='{.spec.registrySources.insecureRegistries}'\nThe output lists the insecure registry sources configured.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "ocp-insecure-registries"
        },
        "creationTimestamp": "2024-03-05T15:10:33Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-bsi",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-bsi-ocp-insecure-registries",
        "namespace": "openshift-compliance",
        "resourceVersion": "82585",
        "uid": "476ceba0-c87b-4a0a-88ca-cca3e5150494"
      },
      "rationale": "Insecure registries should not be configured, which would restrict the possibilities of OpenShift container runtime accessing registries which cannot be validated.",
      "severity": "medium",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Ensure that the RBAC setup follows the principle of least privilege\nRole-based access control (RBAC) objects determine whether a user is allowed to perform a given action within a project. If users or groups exist that are bound to roles they must not have, modify the user or group permissions using the following cluster and local role binding commands: Remove a User from a Cluster RBAC role by executing the following: oc adm policy remove-cluster-role-from-user role username Remove a Group from a Cluster RBAC role by executing the following: oc adm policy remove-cluster-role-from-group role groupname Remove a User from a Local RBAC role by executing the following: oc adm policy remove-role-from-user role username Remove a Group from a Local RBAC role by executing the following: oc adm policy remove-role-from-group role groupname NOTE: For additional information. https://docs.openshift.com/container-platform/latest/authentication/using-rbac.html",
      "id": "xccdf_org.ssgproject.content_rule_rbac_least_privilege",
      "instructions": "The administrator must verify that Openshift is configured with the necessary RBAC access controls.\n\nReview the RBAC configuration.\n\nAs the cluster-admin, view the cluster roles and their associated rule sets by executing the following::\n\noc describe clusterrole.rbac\n\nNow view the current set of cluster role bindings, which shows the users and groups that are bound to various roles by executing the following:\n\noc describe clusterrolebinding.rbac\n\nLocal roles and bindings can be determined using the follow commands by executing the following:\n\noc describe rolebinding.rbac\n\nIf these results show users with privileged access that do not require that access, this is a finding.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "rbac-least-privilege"
        },
        "creationTimestamp": "2024-03-05T15:10:33Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "high",
          "compliance.openshift.io/check-status": "MANUAL",
          "compliance.openshift.io/scan-name": "ocp4-bsi",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-bsi-rbac-least-privilege",
        "namespace": "openshift-compliance",
        "resourceVersion": "82596",
        "uid": "8f210806-5a05-4e8d-99f3-b080a55480d8"
      },
      "rationale": "Controlling and limiting users access to system services and resources is key to securing the platform and limiting the intentional or unintentional comprimising of the system and its services. OpenShift provides a robust RBAC policy system that allows for authorization policies to be as detailed as needed. Additionally there are two layers of RBAC policies, the first is Cluster RBAC policies which administrators can control who has what access to cluster level services. The other is Local RBAC policies, which allow project developers/administrators to control what level of access users have to a given project or namespace.",
      "severity": "high",
      "status": "MANUAL"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Limit Containers Ability to use the HostDir volume plugin\nContainers should be allowed to use the hostPath volume type unless necessary. To prevent containers from using the host filesystem the appropriate Security Context Constraints (SCCs) should set allowHostDirVolumePlugin to false.",
      "id": "xccdf_org.ssgproject.content_rule_scc_limit_host_dir_volume_plugin",
      "instructions": "Inspect each SCC returned from running the following command:\n$ oc get scc\nReview each SCC for those that have allowHostDirVolumePlugin\nset to true.  Next, examine the outputs of the following commands:\n$ oc describe roles --all-namespaces\n$ oc describe clusterroles\nFor any role/clusterrole that reference the\nsecuritycontextconstraints resource with the resourceNames\nof the SCCs that have allowHostDirVolumePlugin, examine the associated\nrolebindings to account for the users that are bound to the role. Review the\naccount to determine if allowHostDirVolumePlugin is truly required.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "scc-limit-host-dir-volume-plugin"
        },
        "creationTimestamp": "2024-03-05T15:10:33Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "MANUAL",
          "compliance.openshift.io/scan-name": "ocp4-bsi",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-bsi-scc-limit-host-dir-volume-plugin",
        "namespace": "openshift-compliance",
        "resourceVersion": "82592",
        "uid": "1ff74d17-6cf9-42f1-b318-fc83a14bd5ec"
      },
      "rationale": "hostPath volumes allow workloads to access the host filesystem from the workload. Access to the host filesystem can be used to escalate privileges and access resources such as keys or access tokens.",
      "severity": "medium",
      "status": "MANUAL"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Limit Access to the Host IPC Namespace\nContainers should not be allowed access to the host's Interprocess Communication (IPC) namespace. To prevent containers from getting access to a host's IPC namespace, the appropriate Security Context Constraints (SCCs) should set allowHostIPC to false.",
      "id": "xccdf_org.ssgproject.content_rule_scc_limit_ipc_namespace",
      "instructions": "Inspect each SCC returned from running the following command:\n$ oc get scc\nReview each SCC for those that have allowHostIPC set to true.\nNext, examine the outputs of the following commands:\n$ oc describe roles --all-namespaces\n$ oc describe clusterroles\nFor any role/clusterrole that reference the\nsecuritycontextconstraints resource with the resourceNames\nof the SCCs that have allowHostIPC, examine the associated\nrolebindings to account for the users that are bound to the role. Review the\naccount to determine if allowHostIPC is truly required.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "scc-limit-ipc-namespace"
        },
        "creationTimestamp": "2024-03-05T15:10:33Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "MANUAL",
          "compliance.openshift.io/scan-name": "ocp4-bsi",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-bsi-scc-limit-ipc-namespace",
        "namespace": "openshift-compliance",
        "resourceVersion": "82597",
        "uid": "69ede89d-2ade-46f5-a122-174faa24f741"
      },
      "rationale": "A container running in the host's IPC namespace can use IPC to interact with processes outside the container potentially allowing an attacker to exploit a host process thereby enabling an attacker to exploit other services.",
      "severity": "medium",
      "status": "MANUAL"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Limit Use of the CAP_NET_RAW\nContainers should not enable more capabilities than needed as this opens the door for malicious use. CAP_NET_RAW enables a container to launch a network attack on another container or cluster. To disable the CAP_NET_RAW capability, the appropriate Security Context Constraints (SCCs) should set NET_RAW in requiredDropCapabilities.",
      "id": "xccdf_org.ssgproject.content_rule_scc_limit_net_raw_capability",
      "instructions": "Inspect each SCC returned from running the following command:\n$ oc get scc\nReview each SCC for those that do not have NET_RAW or ALL set under requiredDropCapabilities.\nNext, examine the outputs of the following commands:\n$ oc describe roles --all-namespaces\n$ oc describe clusterroles\nFor any role/clusterrole that reference the\nsecuritycontextconstraints resource with the resourceNames\nof the SCCs that do not drop NET_RAW or ALL, examine the\nassociated rolebindings to account for the users that are bound to the role.\nReview each SCC and determine that either NET_RAW or ALL\nis either included as a list entry under requiredDropCapabilities,\nor that either NET_RAW or ALL is only enabled to a small\nset of containers and SCCs.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "scc-limit-net-raw-capability"
        },
        "creationTimestamp": "2024-03-05T15:10:33Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "MANUAL",
          "compliance.openshift.io/scan-name": "ocp4-bsi",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-bsi-scc-limit-net-raw-capability",
        "namespace": "openshift-compliance",
        "resourceVersion": "82598",
        "uid": "df80495d-1869-402d-8e45-5c4b95d09775"
      },
      "rationale": "By default, containers run with a default set of capabilities as assigned by the Container Runtime which can include dangerous or highly privileged capabilities. If the CAP_NET_RAW is enabled, it may be misused by malicious containers or attackers.",
      "severity": "medium",
      "status": "MANUAL"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Limit Access to the Host Network Namespace\nContainers should not be allowed access to the host's network namespace. To prevent containers from getting access to a host's network namespace, the appropriate Security Context Constraints (SCCs) should set allowHostNetwork to false.",
      "id": "xccdf_org.ssgproject.content_rule_scc_limit_network_namespace",
      "instructions": "Inspect each SCC returned from running the following command:\n$ oc get scc\nReview each SCC for those that have allowHostNetwork set to true.\nNext, examine the outputs of the following commands:\n$ oc describe roles --all-namespaces\n$ oc describe clusterroles\nFor any role/clusterrole that reference the\nsecuritycontextconstraints resource with the resourceNames\nof the SCCs that have allowHostNetwork, examine the associated\nrolebindings to account for the users that are bound to the role. Review the\naccount to determine if allowHostNetwork is truly required.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "scc-limit-network-namespace"
        },
        "creationTimestamp": "2024-03-05T15:10:33Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "MANUAL",
          "compliance.openshift.io/scan-name": "ocp4-bsi",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-bsi-scc-limit-network-namespace",
        "namespace": "openshift-compliance",
        "resourceVersion": "82586",
        "uid": "6889a92b-d56e-415e-a16e-892fcdfe0adc"
      },
      "rationale": "A container running in the host's network namespace could access the host network traffic to and from other pods potentially allowing an attacker to exploit pods and network traffic.",
      "severity": "medium",
      "status": "MANUAL"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Limit Privileged Container Use\nContainers should be limited to only the privileges required to run. To prevent containers from running as privileged containers, the appropriate Security Context Constraints (SCCs) should set allowPrivilegedContainer to false.",
      "id": "xccdf_org.ssgproject.content_rule_scc_limit_privileged_containers",
      "instructions": "Inspect each SCC and the users and groups allowed to use it returned\nfrom running the following command:\noc get scc -ojson | jq '.items[]|select(.allowPrivilegedContainer)|.metadata.name,{\"Group:\":.groups},{\"User\":.users}'\n\nThe group \"system:authenticated\" is the default group for any\nauthenticated user, this group should only be associated with the\nrestricted profile. If this group is listed under any other SCC Policy,\nor the restricted SCC policy has been altered to allow any of the\nnon-permitted actions, this is a finding.\n\nNext, determine if there are any cluster roles or local roles that allow\nthe use of use of non-permitted SCC policies.  The following commands will\nprint the Role's name and namespace, followed by a list of resource names\nand if that resource is an SCC.\n\n> oc get clusterrole.rbac -ojson | jq -r '.items[]|select(.rules[]?|select( (.apiGroups[]? == (\"security.openshift.io\")) and (.resources[]? == (\"securitycontextconstraints\")) and (.verbs[]? == (\"use\"))))|.metadata.name,{\"scc\":(.rules[]?|select((.resources[]? == (\"securitycontextconstraints\"))).resourceNames[]?)}'\n\n> oc get role.rbac --all-namespaces -ojson | jq -r '.items[]|select(.rules[]?|select( (.apiGroups[]? == (\"security.openshift.io\")) and (.resources[]? == (\"securitycontextconstraints\")) and (.verbs[]? == (\"use\"))))|.metadata.name,{\"scc\":(.rules[]?|select((.resources[]? == (\"securitycontextconstraints\"))).resourceNames[]?)}'\n\nExcluding platform specific roles, identify any roles that allow use of non-permitted SCC policies for example the follow output shows that the role 'examplePrivilegedRole' allows use of the 'privileged' SCC.\n\n\nexamplePrivilegedRole\n{\n  \"scc\": \"privileged\"\n}\n\n\nFinally, determine if there are any role bindings to cluster or local\nroles that allow use of non-permitted SCCs.\n\n> oc get clusterrolebinding.rbac -ojson | jq -r '.items[]|select(.roleRef.kind == (\"ClusterRole\",\"Role\") and .roleRef.name == (\"[CLUSTER_ROLE_LIST]\"))|{ \"crb\": .metadata.name, \"roleRef\": .roleRef, \"subjects\": .subjects}'\n> oc get rolebinding.rbac --all-namespaces -ojson | jq -r '.items[]|select(.roleRef.kind == (\"ClusterRole\",\"Role\") and .roleRef.name == (\"[LOCAL_ROLE_LIST]\"))|{ \"crb\": .metadata.name, \"roleRef\": .roleRef, \"subjects\": .subjects}'\n\nWhere \"[CLUSTER_ROLE_LIST]\" and \"[LOCAL_ROLE_LIST]\" are\ncomma-separated lists of the roles allowing use of non-permitted SCC\npolicies as identified above. For example:\n\n\n... .roleRef.name == (\"system:openshift:scc:privileged\",\"system:openshift:scc:hostnetwork\",\"system:openshift:scc:hostaccess\") ...\n\n\nExcluding any platform namespaces (kube-*,openshift-*), if there are any rolebindings to roles that are not permitted, this is a finding.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "scc-limit-privileged-containers"
        },
        "creationTimestamp": "2024-03-05T15:10:33Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "MANUAL",
          "compliance.openshift.io/scan-name": "ocp4-bsi",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-bsi-scc-limit-privileged-containers",
        "namespace": "openshift-compliance",
        "resourceVersion": "82594",
        "uid": "c58ae688-0e6b-46e8-bfe1-84801fdf7a31"
      },
      "rationale": "Privileged containers have access to all Linux Kernel capabilities and devices. If a privileged container were compromised, an attacker would have full access to the container and host.",
      "severity": "medium",
      "status": "MANUAL"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Limit Access to the Host Process ID Namespace\nContainers should not be allowed access to the host's process ID namespace. To prevent containers from getting access to a host's process ID namespace, the appropriate Security Context Constraints (SCCs) should set allowHostPID to false.",
      "id": "xccdf_org.ssgproject.content_rule_scc_limit_process_id_namespace",
      "instructions": "Inspect each SCC returned from running the following command:\n$ oc get scc\nReview each SCC for those that have allowHostPID set to true.\nNext, examine the outputs of the following commands:\n$ oc describe roles --all-namespaces\n$ oc describe clusterroles\nFor any role/clusterrole that reference the\nsecuritycontextconstraints resource with the resourceNames\nof the SCCs that have allowHostPID, examine the associated\nrolebindings to account for the users that are bound to the role. Review the\naccount to determine if allowHostPID is truly required.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "scc-limit-process-id-namespace"
        },
        "creationTimestamp": "2024-03-05T15:10:33Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "MANUAL",
          "compliance.openshift.io/scan-name": "ocp4-bsi",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-bsi-scc-limit-process-id-namespace",
        "namespace": "openshift-compliance",
        "resourceVersion": "82587",
        "uid": "33e11c69-254c-410d-b4d9-026cd4b4120e"
      },
      "rationale": "A container running in the host's PID namespace can inspect processes running outside the container which can be used to escalate privileges outside of the container.",
      "severity": "medium",
      "status": "MANUAL"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Limit Container Running As Root User\nContainers should run as a random non-privileged user. To prevent containers from running as root user, the appropriate Security Context Constraints (SCCs) should set.runAsUser.type to MustRunAsRange.",
      "id": "xccdf_org.ssgproject.content_rule_scc_limit_root_containers",
      "instructions": "Inspect each SCC and the users and groups allowed to use it returned\nfrom running the following command:\noc get scc -ojson | jq '.items[]|select(.runAsUser.type != \"MustRunAsRange\" )|.metadata.name,{\"Group:\":.groups},{\"User\":.users}'\n\nThe group \"system:authenticated\" is the default group for any\nauthenticated user, this group should only be associated with the\nrestricted profile. If this group is listed under any other SCC Policy,\nor the restricted SCC policy has been altered to allow any of the\nnon-permitted actions, this is a finding.\n\nNext, determine if there are any cluster roles or local roles that allow\nthe use of use of non-permitted SCC policies.  The following commands will\nprint the Role's name and namespace, followed by a list of resource names\nand if that resource is an SCC.\n\n> oc get clusterrole.rbac -ojson | jq -r '.items[]|select(.rules[]?|select( (.apiGroups[]? == (\"security.openshift.io\")) and (.resources[]? == (\"securitycontextconstraints\")) and (.verbs[]? == (\"use\"))))|.metadata.name,{\"scc\":(.rules[]?|select((.resources[]? == (\"securitycontextconstraints\"))).resourceNames[]?)}'\n\n> oc get role.rbac --all-namespaces -ojson | jq -r '.items[]|select(.rules[]?|select( (.apiGroups[]? == (\"security.openshift.io\")) and (.resources[]? == (\"securitycontextconstraints\")) and (.verbs[]? == (\"use\"))))|.metadata.name,{\"scc\":(.rules[]?|select((.resources[]? == (\"securitycontextconstraints\"))).resourceNames[]?)}'\n\nExcluding platform specific roles, identify any roles that allow use of non-permitted SCC policies for example the follow output shows that the role 'examplePrivilegedRole' allows use of the 'privileged' SCC.\n\n\nexamplePrivilegedRole\n{\n  \"scc\": \"privileged\"\n}\n\n\nFinally, determine if there are any role bindings to cluster or local\nroles that allow use of non-permitted SCCs.\n\n> oc get clusterrolebinding.rbac -ojson | jq -r '.items[]|select(.roleRef.kind == (\"ClusterRole\",\"Role\") and .roleRef.name == (\"[CLUSTER_ROLE_LIST]\"))|{ \"crb\": .metadata.name, \"roleRef\": .roleRef, \"subjects\": .subjects}'\n> oc get rolebinding.rbac --all-namespaces -ojson | jq -r '.items[]|select(.roleRef.kind == (\"ClusterRole\",\"Role\") and .roleRef.name == (\"[LOCAL_ROLE_LIST]\"))|{ \"crb\": .metadata.name, \"roleRef\": .roleRef, \"subjects\": .subjects}'\n\nWhere \"[CLUSTER_ROLE_LIST]\" and \"[LOCAL_ROLE_LIST]\" are\ncomma-separated lists of the roles allowing use of non-permitted SCC\npolicies as identified above. For example:\n\n\n... .roleRef.name == (\"system:openshift:scc:privileged\",\"system:openshift:scc:hostnetwork\",\"system:openshift:scc:hostaccess\") ...\n\n\nExcluding any platform namespaces (kube-*,openshift-*), if there are any rolebindings to roles that are not permitted, this is a finding.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "scc-limit-root-containers"
        },
        "creationTimestamp": "2024-03-05T15:10:33Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "MANUAL",
          "compliance.openshift.io/scan-name": "ocp4-bsi",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-bsi-scc-limit-root-containers",
        "namespace": "openshift-compliance",
        "resourceVersion": "82588",
        "uid": "57a9e7b5-ee3d-4b6d-96ca-ed30306a8c74"
      },
      "rationale": "It is strongly recommended that containers running on OpenShift should support running as any arbitrary UID. OpenShift will then assign a random, non-privileged UID to the running container instance. This avoids the risk from containers running with specific uids that could map to host service accounts, or an even greater risk of running as root level service. OpenShift uses the default security context constraints (SCC), restricted, to prevent containers from running as root or other privileged user ids. Pods may be configured to use an scc policy that allows the container to run as a specific uid, including root(0) when approved. Only a cluster administrator may grant the change of an scc policy.",
      "severity": "medium",
      "status": "MANUAL"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Restrict Automounting of Service Account Tokens\nService accounts tokens should not be mounted in pods except where the workload running in the pod explicitly needs to communicate with the API server. To ensure pods do not automatically mount tokens, set automountServiceAccountToken to false.",
      "id": "xccdf_org.ssgproject.content_rule_accounts_restrict_service_account_tokens",
      "instructions": "For each pod in the cluster, review the pod specification and\nensure that pods that do not need to explicitly communicate with\nthe API server have automountServiceAccountToken\nconfigured to false.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "accounts-restrict-service-account-tokens"
        },
        "creationTimestamp": "2024-03-05T15:10:50Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "MANUAL",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-accounts-restrict-service-account-tokens",
        "namespace": "openshift-compliance",
        "resourceVersion": "82826",
        "uid": "e3e31055-ac8a-40e6-9090-23373e01a3ff"
      },
      "rationale": "Mounting service account tokens inside pods can provide an avenue for privilege escalation attacks where an attacker is able to compromise a single pod in the cluster.",
      "severity": "medium",
      "status": "MANUAL"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Ensure Usage of Unique Service Accounts \nUsing the default service account prevents accurate application rights review and audit tracing. Instead of default , create a new and unique service account with the following command:\n\n$ oc create sa service_account_name\n\nwhere service_account_name is the name of a service account that is needed in the project namespace.",
      "id": "xccdf_org.ssgproject.content_rule_accounts_unique_service_account",
      "instructions": "For each namespace in the cluster, review the rights assigned\nto the default service account. There should be no cluster or local roles\nassigned to the default other than the defaults.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "accounts-unique-service-account"
        },
        "creationTimestamp": "2024-03-05T15:10:48Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "MANUAL",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-accounts-unique-service-account",
        "namespace": "openshift-compliance",
        "resourceVersion": "82787",
        "uid": "14ec05d2-38b7-4947-9c12-44ab4708e0b5"
      },
      "rationale": "Kubernetes provides a default service account which is used by cluster workloads where no specific service account is assigned to the pod. Where access to the Kubernetes API from a pod is required, a specific service account should be created for that pod, and rights granted to that service account. This increases auditability of service account rights and access making it easier and more accurate to trace potential malicious behaviors to a specific service account and project.",
      "severity": "medium",
      "status": "MANUAL"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Disable the AlwaysAdmit Admission Control Plugin\nTo ensure OpenShift only responses to requests explicitly allowed by the admission control plugin. Check that the config ConfigMap object does not contain the AlwaysAdmit plugin.",
      "id": "xccdf_org.ssgproject.content_rule_api_server_admission_control_plugin_alwaysadmit",
      "instructions": "To verify that the AlwaysAdmit admission control plugin is not set, run the following command:\n$ oc -n openshift-kube-apiserver get configmap config -o json | jq -r '.data.\"config.yaml\"' | jq '.apiServerArguments.\"enable-admission-plugins\"'\nThe output should not contain AlwaysAdmit",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "api-server-admission-control-plugin-alwaysadmit"
        },
        "creationTimestamp": "2024-03-05T15:10:51Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-api-server-admission-control-plugin-alwaysadmit",
        "namespace": "openshift-compliance",
        "resourceVersion": "82831",
        "uid": "3f5d63ac-6361-4058-ac2a-71d83bbab255"
      },
      "rationale": "Enabling the admission control plugin AlwaysAdmit allows all requests and does not provide any filtering.",
      "severity": "medium",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Ensure that the Admission Control Plugin AlwaysPullImages is not set\nThe AlwaysPullImages admission control plugin should be disabled, since it can introduce new failure modes for control plane components if an image registry is unreachable.",
      "id": "xccdf_org.ssgproject.content_rule_api_server_admission_control_plugin_alwayspullimages",
      "instructions": "Run the following command:\n$ oc -n openshift-kube-apiserver get configmap config -o json | jq -r '.data.\"config.yaml\"' | jq '.apiServerArguments.\"enable-admission-plugins\"'\nThe output list should not contain \"AlwaysPullImages\".",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "api-server-admission-control-plugin-alwayspullimages"
        },
        "creationTimestamp": "2024-03-05T15:10:47Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "high",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-api-server-admission-control-plugin-alwayspullimages",
        "namespace": "openshift-compliance",
        "resourceVersion": "82764",
        "uid": "a9fdfd62-9c92-4735-a315-6e70d50de319"
      },
      "rationale": "Setting admission control policy to AlwaysPullImages forces every new pod to pull the required images every time. In a multi-tenant cluster users can be assured that their private images can only be used by those who have the credentials to pull them. Without this admission control policy, once an image has been pulled to a node, any pod from any user can use it simply by knowing the imageâ€™s name, without any authorization check against the image ownership. When this plug-in is enabled, images are always pulled prior to starting containers, which means valid credentials are required. However, turning on this admission plugin can introduce new kinds of cluster failure modes. OpenShift 4 master and infrastructure components are deployed as pods. Enabling this feature can result in cases where loss of contact to an image registry can cause a redeployed infrastructure pod (oauth-server for example) to fail on an image pull for an image that is currently present on the node. We use PullIfNotPresent so that a loss of image registry access does not prevent the pod from starting. If it becomes PullAlways, then an image registry access outage can cause key infrastructure components to fail. The pull policy can be managed per container, using imagePullPolicy.",
      "severity": "high",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Enable the NamespaceLifecycle Admission Control Plugin\nOpenShift enables the NamespaceLifecycle plugin by default.",
      "id": "xccdf_org.ssgproject.content_rule_api_server_admission_control_plugin_namespacelifecycle",
      "instructions": "To verify that the NamespaceLifecycle plugin is enabled in\nthe apiserver configuration, run:\n$ oc -n openshift-kube-apiserver get configmap config -o json | jq -r '.data.\"config.yaml\"' | jq '.apiServerArguments.\"enable-admission-plugins\"'",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "api-server-admission-control-plugin-namespacelifecycle"
        },
        "creationTimestamp": "2024-03-05T15:10:43Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-api-server-admission-control-plugin-namespacelifecycle",
        "namespace": "openshift-compliance",
        "resourceVersion": "82716",
        "uid": "6e7783f0-925c-4f4e-9232-404205a12d63"
      },
      "rationale": "Setting admission control policy to NamespaceLifecycle ensures that objects cannot be created in non-existent namespaces, and that namespaces undergoing termination are not used for creating new objects. This is recommended to enforce the integrity of the namespace termination process and also for the availability of new objects.",
      "severity": "medium",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Enable the NodeRestriction Admission Control Plugin\nTo limit the Node and Pod objects that a kubelet could modify, ensure that the NodeRestriction plugin on kubelets is enabled in the api-server configuration by running the following command:\n\n$ oc -n openshift-kube-apiserver get configmap config -o json | jq -r '.data.\"config.yaml\"' | jq '.apiServerArguments.\"enable-admission-plugins\"'",
      "id": "xccdf_org.ssgproject.content_rule_api_server_admission_control_plugin_noderestriction",
      "instructions": "Ensure that the NodeRestriction plugin is enabled in the list of enabled plugins in\nthe apiserver configuration by running the following command:\n$ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data[\"config.yaml\"]' | grep 'NodeRestriction'",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "api-server-admission-control-plugin-noderestriction"
        },
        "creationTimestamp": "2024-03-05T15:10:43Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-api-server-admission-control-plugin-noderestriction",
        "namespace": "openshift-compliance",
        "resourceVersion": "82712",
        "uid": "596d5d1d-a567-4f45-921f-dff0f9df8783"
      },
      "rationale": "Using the NodeRestriction plugin ensures that the kubelet is restricted to the Node and Pod objects that it could modify as defined. Such kubelets will only be allowed to modify their own Node API object, and only modify Pod API objects that are bound to their node.",
      "severity": "medium",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Enable the SecurityContextConstraint Admission Control Plugin\nTo ensure pod permissions are managed, make sure that the SecurityContextConstraint admission control plugin is used.",
      "id": "xccdf_org.ssgproject.content_rule_api_server_admission_control_plugin_scc",
      "instructions": "The SecurityContextConstraint plugin should be enabled in the list of enabled plugins in\nthe apiserver configuration:\n$ oc -n openshift-kube-apiserver get configmap config -o json | jq -r '.data.\"config.yaml\"' | jq '.apiServerArguments.\"enable-admission-plugins\"'",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "api-server-admission-control-plugin-scc"
        },
        "creationTimestamp": "2024-03-05T15:10:44Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-api-server-admission-control-plugin-scc",
        "namespace": "openshift-compliance",
        "resourceVersion": "82733",
        "uid": "2f755925-f146-41df-aa9b-2fcb9ca08ce8"
      },
      "rationale": "A Security Context Constraint is a cluster-level resource that controls the actions which a pod can perform and what the pod may access. The SecurityContextConstraint objects define a set of conditions that a pod must run with in order to be accepted into the system. Security Context Constraints are comprised of settings and strategies that control the security features a pod has access to and hence this must be used to control pod access permissions.",
      "severity": "medium",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Enable the ServiceAccount Admission Control Plugin\nTo ensure ServiceAccount objects must be created and granted before pod creation is allowed, follow the documentation and create ServiceAccount objects as per your environment. Ensure that the plugin is enabled in the api-server configuration:\n\n$ oc -n openshift-kube-apiserver get configmap config -o json | jq -r '.data.\"config.yaml\"' | jq '.apiServerArguments.\"enable-admission-plugins\"'",
      "id": "xccdf_org.ssgproject.content_rule_api_server_admission_control_plugin_service_account",
      "instructions": "The ServiceAccount plugin should be enabled in the list of enabled plugins in\nthe apiserver configuration:\n$ oc -n openshift-kube-apiserver get configmap config -o json | jq -r '.data.\"config.yaml\"' | jq '.apiServerArguments.\"enable-admission-plugins\"'",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "api-server-admission-control-plugin-service-account"
        },
        "creationTimestamp": "2024-03-05T15:10:47Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-api-server-admission-control-plugin-service-account",
        "namespace": "openshift-compliance",
        "resourceVersion": "82771",
        "uid": "ae12fc58-8b2a-4b3a-8902-a896b3c2ac83"
      },
      "rationale": "When a pod is created, if a service account is not specified, the pod is automatically assigned the default service account in the same namespace. OpenShift operators should create unique service accounts and let the API Server manage its security tokens.",
      "severity": "medium",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Ensure that anonymous requests to the API Server are authorized\nBy default, anonymous access to the OpenShift API is enabled, but at the same time, all requests must be authorized. If no authentication mechanism is used, the request is assigned the system:anonymous virtual user and the system:unauthenticated virtual group. This allows the authorization layer to determine which requests, if any, is an anonymous user authorized to make. To verify the authorization rules for anonymous requests run the following:\n\n$ oc describe clusterrolebindings\n\nand inspect the bindings of the system:anonymous virtual user and the system:unauthenticated virtual group. To test that an anonymous request is authorized to access the readyz endpoint, run:\n\n$ oc get --as=\"system:anonymous\" --raw='/readyz?verbose'\n\nIn contrast, a request to list all projects should not be authorized:\n\n$ oc get --as=\"system:anonymous\" projects",
      "id": "xccdf_org.ssgproject.content_rule_api_server_anonymous_auth",
      "instructions": "Run the following command to view the authorization rules for anonymous requests:\n$ oc describe clusterrolebindings\nMake sure that there exists at least one clusterrolebinding that binds\neither the system:unauthenticated group or the system:anonymous\nuser.\nTo test that an anonymous request is authorized to access the readyz\nendpoint, run:\n$ oc get --as=\"system:anonymous\" --raw='/readyz?verbose'\nIn contrast, a request to list all projects should not be authorized:\n$ oc get --as=\"system:anonymous\" projects",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "api-server-anonymous-auth"
        },
        "creationTimestamp": "2024-03-05T15:10:44Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-api-server-anonymous-auth",
        "namespace": "openshift-compliance",
        "resourceVersion": "82728",
        "uid": "cc793cf5-d5f4-49c8-8715-13984f97edb9"
      },
      "rationale": "When enabled, requests that are not rejected by other configured authentication methods are treated as anonymous requests. These requests are then served by the API server. If you are using RBAC authorization, it is generally considered reasonable to allow anonymous access to the API Server for health checks and discovery purposes, and hence this recommendation is not scored. However, you should consider whether anonymous discovery is an acceptable risk for your purposes.",
      "severity": "medium",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Enable the APIPriorityAndFairness feature gate\nTo limit the rate at which the API Server accepts requests, make sure that the API Priority and Fairness feature is enabled. Using APIPriorityAndFairness feature provides a fine-grained way to control the behaviour of the Kubernetes API server in an overload situation. To enable the APIPriorityAndFairness feature gate, make sure that the feature-gates API server argument, typically set in the config configMap in the openshift-kube-apiserver namespace contains APIPriorityAndFairness=true. Note that since Kubernetes 1.20, this feature gate is enabled by default. As a result, this rule is only applicable to OpenShift releases prior to 4.7 which was the first OCP release to ship Kubernetes 1.20.",
      "id": "xccdf_org.ssgproject.content_rule_api_server_api_priority_gate_enabled",
      "instructions": "To verify that APIPriorityAndFairness is enabled, run the following command:\noc get kubeapiservers.operator.openshift.io cluster -o json | jq '.spec.observedConfig.apiServerArguments[\"feature-gates\"]'\nThe output should contain \"APIPriorityAndFairness=true\"",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "api-server-api-priority-gate-enabled"
        },
        "creationTimestamp": "2024-03-05T15:10:48Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "FAIL",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-api-server-api-priority-gate-enabled",
        "namespace": "openshift-compliance",
        "resourceVersion": "82779",
        "uid": "fa4ae8ba-5162-4a8c-8ab3-76683896b2d5"
      },
      "rationale": "The APIPriorityAndFairness feature gate enables the use of the FlowSchema API objects which enforce a limit on the number of events that the API Server will accept in a given time slice In a large multi-tenant cluster, there might be a small percentage of misbehaving tenants which could have a significant impact on the performance of the cluster overall. It is recommended to limit the rate of events that the API Server will accept.",
      "severity": "medium",
      "status": "FAIL"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Configure the Kubernetes API Server Maximum Retained Audit Logs\nTo configure how many rotations of audit logs are retained, edit the openshift-kube-apiserver configmap and set the audit-log-maxbackup parameter to 10 or to an organizationally appropriate value:\n\n\"apiServerArguments\":{\n ...\n \"audit-log-maxbackup\": [10],\n ...",
      "id": "xccdf_org.ssgproject.content_rule_api_server_audit_log_maxbackup",
      "instructions": "Run the following command:\n$ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data[\"config.yaml\"]' | jq '.apiServerArguments[\"audit-log-maxbackup\"][0]'\nThe output should return a value of 10 or as appropriate.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "api-server-audit-log-maxbackup"
        },
        "creationTimestamp": "2024-03-05T15:10:43Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "low",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-api-server-audit-log-maxbackup",
        "namespace": "openshift-compliance",
        "resourceVersion": "82700",
        "uid": "bee6197b-6498-47a8-a41f-74bdc0e7465b"
      },
      "rationale": "OpenShift automatically rotates the log files. Retaining old log files ensures OpenShift Operators will have sufficient log data available for carrying out any investigation or correlation. For example, if the audit log size is set to 100 MB and the number of retained log files is set to 10, OpenShift Operators would have approximately 1 GB of log data to use during analysis.",
      "severity": "low",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Configure Kubernetes API Server Maximum Audit Log Size\nTo rotate audit logs upon reaching a maximum size, edit the openshift-kube-apiserver configmap and set the audit-log-maxsize parameter to an appropriate size in MB. For example, to set it to 100 MB:\n\n\"apiServerArguments\":{\n ...\n \"audit-log-maxsize\": [\"100\"],\n ...",
      "id": "xccdf_org.ssgproject.content_rule_api_server_audit_log_maxsize",
      "instructions": "Run the following command:\n$ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data[\"config.yaml\"]' | jq '.apiServerArguments[\"audit-log-maxsize\"]'\nThe output should return a value of [\"100\"] or as appropriate.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "api-server-audit-log-maxsize"
        },
        "creationTimestamp": "2024-03-05T15:10:49Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-api-server-audit-log-maxsize",
        "namespace": "openshift-compliance",
        "resourceVersion": "82814",
        "uid": "34e43932-2a24-4050-8147-3bb9c3ff4ce9"
      },
      "rationale": "OpenShift automatically rotates log files. Retaining old log files ensures that OpenShift Operators have sufficient log data available for carrying out any investigation or correlation. If you have set file size of 100 MB and the number of old log files to keep as 10, there would be approximately 1 GB of log data available for use in analysis.",
      "severity": "medium",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Configure the Audit Log Path\nTo enable auditing on the Kubernetes API Server, the audit log path must be set. Edit the openshift-kube-apiserver configmap and set the audit-log-path to a suitable path and file where audit logs should be written. For example:\n\n\"apiServerArguments\":{\n ...\n \"audit-log-path\":\"/var/log/kube-apiserver/audit.log\",\n ...",
      "id": "xccdf_org.ssgproject.content_rule_api_server_audit_log_path",
      "instructions": "Run the following command:\n$ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data[\"config.yaml\"]' | jq '.apiServerArguments[\"audit-log-path\"]'\nThe output should return a valid audit log path.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "api-server-audit-log-path"
        },
        "creationTimestamp": "2024-03-05T15:10:43Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "high",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-api-server-audit-log-path",
        "namespace": "openshift-compliance",
        "resourceVersion": "82720",
        "uid": "bba3edef-2d1a-4e5a-93a9-6a99dbebc59b"
      },
      "rationale": "Auditing of the Kubernetes API Server is not enabled by default. Auditing the API Server provides a security-relevant chronological set of records documenting the sequence of activities that have affected the system by users, administrators, or other system components.",
      "severity": "high",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "The authorization-mode cannot be AlwaysAllow\nDo not always authorize all requests.",
      "id": "xccdf_org.ssgproject.content_rule_api_server_auth_mode_no_aa",
      "instructions": "To verify that the Node authorization mode is be configured and enabled in\nthe apiserver configuration, run:\n$ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data[\"config.yaml\"]' | grep '\"authorization-mode\":\\[[^]]*\"AlwaysAllow\"'\nThe output should be empty - the \"authorization-mode\" list does NOT contain the \"AlwaysAllow\" authorizer.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "api-server-auth-mode-no-aa"
        },
        "creationTimestamp": "2024-03-05T15:10:44Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-api-server-auth-mode-no-aa",
        "namespace": "openshift-compliance",
        "resourceVersion": "82730",
        "uid": "11a956e2-c330-41af-9500-0e5d08b81556"
      },
      "rationale": "The API Server, can be configured to allow all requests. This mode should not be used on any production cluster.",
      "severity": "medium",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Ensure authorization-mode RBAC is configured\nTo ensure OpenShift restricts different identities to a defined set of operations they are allowed to perform, check that the API server's authorization-mode configuration option list contains RBAC.",
      "id": "xccdf_org.ssgproject.content_rule_api_server_auth_mode_rbac",
      "instructions": "To verify that RBAC authorization mode is enabled, run the following command:\n$ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data[\"config.yaml\"]' | grep '\"authorization-mode\":\\[[^]]*\"RBAC\"'\nThe output should show that the \"authorization-mode\" list contains the \"RBAC\" authorizer.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "api-server-auth-mode-rbac"
        },
        "creationTimestamp": "2024-03-05T15:10:48Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-api-server-auth-mode-rbac",
        "namespace": "openshift-compliance",
        "resourceVersion": "82781",
        "uid": "4dc5c21c-6941-48fb-94e9-37f74975aa69"
      },
      "rationale": "Role Based Access Control (RBAC) allows fine-grained control over the operations that different entities can perform on different objects in the cluster. Enabling RBAC is critical in regulating access to an OpenShift cluster as the RBAC rules specify, given a user, which operations can be executed over a set of namespaced or cluster-wide resources.",
      "severity": "medium",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Disable basic-auth-file for the API Server\nBasic Authentication should not be used for any reason. If needed, edit API Edit the openshift-kube-apiserver configmap and remove the basic-auth-file parameter:\n\n\"apiServerArguments\":{\n ...\n \"basic-auth-file\":[\n   \"/path/to/any/file\"\n ],\n ...\n\nAlternate authentication mechanisms such as tokens and certificates will need to be used. Username and password for basic authentication will be disabled.",
      "id": "xccdf_org.ssgproject.content_rule_api_server_basic_auth",
      "instructions": "To verify that basic-auth-file is configured and enabled for the API server, run the following command:\n$ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data[\"config.yaml\"]' | jq '.apiServerArguments[\"basic-auth-file\"]'\nThe output should be empty as OpenShift does not support basic authentication at all.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "api-server-basic-auth"
        },
        "creationTimestamp": "2024-03-05T15:10:46Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-api-server-basic-auth",
        "namespace": "openshift-compliance",
        "resourceVersion": "82755",
        "uid": "86d6ed1a-2412-4935-821b-446298dac669"
      },
      "rationale": "Basic authentication uses plaintext credentials for authentication. Currently the basic authentication credentials last indefinitely, and the password cannot be changed without restarting the API Server. The Basic Authentication is currently supported for convenience and is not intended for production workloads.",
      "severity": "medium",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Ensure that the bindAddress is set to a relevant secure port\nThe bindAddress is set by default to 0.0.0.0:6443 , and listening with TLS enabled.",
      "id": "xccdf_org.ssgproject.content_rule_api_server_bind_address",
      "instructions": "Run the following command:\noc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data[\"config.yaml\"]' | jq -r '.servingInfo[\"bindAddress\"]'\nThe output should return 0.0.0.0:6443.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "api-server-bind-address"
        },
        "creationTimestamp": "2024-03-05T15:10:43Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-has-value": "",
          "compliance.openshift.io/check-severity": "low",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-api-server-bind-address",
        "namespace": "openshift-compliance",
        "resourceVersion": "82701",
        "uid": "d6c9fff4-9697-4c30-85b0-34eaa971a122"
      },
      "rationale": "The OpenShift API server is served over HTTPS with authentication and authorization; the secure API endpoint is bound to 0.0.0.0:6443 by default. In OpenShift, the only supported way to access the API server pod is through the load balancer and then through the internal service. The value is set by the bindAddress argument under the servingInfo parameter.",
      "severity": "low",
      "status": "PASS",
      "valuesUsed": [
        "var-apiserver-bind-address"
      ]
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Configure the Client Certificate Authority for the API Server\nCertificates must be provided to fully setup TLS client certificate authentication. To ensure the API Server utilizes its own TLS certificates, the clientCA must be configured. Verify that servingInfo has the clientCA configured in the openshift-kube-apiserver config configmap to something similar to:\n\n\"apiServerArguments\": {\n ...\n   \"client-ca-file\": [\n     \"/etc/kubernetes/static-pod-certs/configmaps/client-ca/ca-bundle.crt\"\n   ],\n ...",
      "id": "xccdf_org.ssgproject.content_rule_api_server_client_ca",
      "instructions": "Run the following command:\n$ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data[\"config.yaml\"]' | jq '.apiServerArguments[\"client-ca-file\"]'\nThe output should return a configured TLS CA certificate file.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "api-server-client-ca"
        },
        "creationTimestamp": "2024-03-05T15:10:45Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-api-server-client-ca",
        "namespace": "openshift-compliance",
        "resourceVersion": "82740",
        "uid": "73489eb2-1c3a-4d22-8e90-54fea9a16a1c"
      },
      "rationale": "API Server communication contains sensitive parameters that should remain encrypted in transit. Configure the API Server to serve only HTTPS traffic. If -clientCA is set, any request presenting a client certificate signed by one of the authorities in the client-ca-file is authenticated with an identity corresponding to the CommonName of the client certificate.",
      "severity": "medium",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Configure the Encryption Provider Cipher\nWhen you enable etcd encryption, the following OpenShift API server and Kubernetes API server resources are encrypted:\n\n* Secrets\n* ConfigMaps\n* Routes\n* OAuth access tokens\n* OAuth authorize tokens\n\nWhen you enable etcd encryption, encryption keys are created. These keys are rotated on a weekly basis. You must have these keys in order to restore from an etcd backup.\n\nTo ensure the correct cipher, set the encryption type aescbc in the apiserver object which configures the API server itself.\n\nspec:\n encryption:\n   type: aescbc\n\nFor more information, follow the relevant documentation ( https://docs.openshift.com/container-platform/latest/security/encrypting-etcd.html ).",
      "id": "xccdf_org.ssgproject.content_rule_api_server_encryption_provider_cipher",
      "instructions": "OpenShift supports encryption of data at rest of etcd datastore, but it is up to the\ncustomer to configure. The asecbc cipher is used. No other ciphers are supported. Keys\nare stored on the filesystem of the master and automatically rotated.\nRun the following command to review the Encrypted status condition for the OpenShift\nAPI server to verify that its resources were successfully encrypted:\n\n# encrypt the etcd datastore\n$ oc get openshiftapiserver -o=jsonpath='{range.items[0].status.conditions[?(@.type==\"Encrypted\")]}{.reason}{\"\\n\"}{.message}{\"\\n\"}'\n\nThe output shows EncryptionCompleted upon successful encryption.\nIf the output shows EncryptionInProgress this means that encryption is still in\nprogress. Wait a few minutes and try again.\nTo display the encryption configured, run the following command:\n$ oc get --raw /apis/config.openshift.io/v1/apiservers/cluster | jq [.spec.encryption.type] \nIf the output does not list aescbc, the encryption is not configured correctly.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "api-server-encryption-provider-cipher"
        },
        "creationTimestamp": "2024-03-05T15:10:49Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/automated-remediation": "",
          "compliance.openshift.io/check-has-value": "",
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "FAIL",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-api-server-encryption-provider-cipher",
        "namespace": "openshift-compliance",
        "resourceVersion": "82806",
        "uid": "80c5103e-7dfe-43bf-80e6-60a67482f233"
      },
      "rationale": "etcd is a highly available key-value store used by OpenShift deployments for persistent storage of all REST API objects. These objects are sensitive in nature and should be encrypted at rest to avoid any disclosures. Where etcd encryption is used, it is important to ensure that the appropriate set of encryption providers is used. Currently, aescbc is the only type supported by OCP.",
      "severity": "medium",
      "status": "FAIL",
      "valuesUsed": [
        "var_apiserver_encryption_path",
        "var_apiserver_encryption_filter"
      ]
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Configure the etcd Certificate Authority for the API Server\nTo ensure etcd is configured to make use of TLS encryption for client connections, follow the OpenShift documentation and setup the TLS connection between the API Server and etcd. Then, verify that apiServerArguments has the etcd-cafile configured in the openshift-kube-apiserver config configmap to something similar to:\n\n\"apiServerArguments\": {\n ...\n   \"etcd-cafile\": [\n       \"/etc/kubernetes/static-pod-resources/configmaps/etcd-serving-ca/ca-bundle.crt\"\n   ],\n ...",
      "id": "xccdf_org.ssgproject.content_rule_api_server_etcd_ca",
      "instructions": "Run the following command:\n$ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data[\"config.yaml\"]' | jq '.apiServerArguments[\"etcd-cafile\"]'\nThe output should return a configured CA certificate for ETCD.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "api-server-etcd-ca"
        },
        "creationTimestamp": "2024-03-05T15:10:49Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-api-server-etcd-ca",
        "namespace": "openshift-compliance",
        "resourceVersion": "82795",
        "uid": "6cbfad08-3fd6-4177-85e3-4102373dfd48"
      },
      "rationale": "etcd is a highly-available key-value store used by OpenShift deployments for persistent storage of all REST API objects. These objects are sensitive in nature and should be protected by client authentication. This requires the API Server to identify itself to the etcd server using a SSL Certificate Authority file.",
      "severity": "medium",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Configure the etcd Certificate for the API Server\nTo ensure etcd is configured to make use of TLS encryption for client communications, follow the OpenShift documentation and setup the TLS connection between the API Server and etcd. Then, verify that apiServerArguments has the etcd-certfile configured in the openshift-kube-apiserver configmap to something similar to:\n\n...\n\"etcd-certfile\": [\n   \"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.crt\"\n],\n...",
      "id": "xccdf_org.ssgproject.content_rule_api_server_etcd_cert",
      "instructions": "Run the following command:\n$ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data[\"config.yaml\"]' | jq -r '.apiServerArguments.\"etcd-certfile\"'\nThe output should return a configured certificate file.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "api-server-etcd-cert"
        },
        "creationTimestamp": "2024-03-05T15:10:47Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-api-server-etcd-cert",
        "namespace": "openshift-compliance",
        "resourceVersion": "82763",
        "uid": "6fd62d0d-61d5-4928-85af-180b19576b2f"
      },
      "rationale": "etcd is a highly-available key-value store used by OpenShift deployments for persistent storage of all REST API objects. These objects are sensitive in nature and should be protected by client authentication. This requires the API Server to identify itself to the etcd server using a client certificate and key.",
      "severity": "medium",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Configure the etcd Certificate Key for the API Server\nTo ensure etcd is configured to make use of TLS encryption for client communications, follow the OpenShift documentation and setup the TLS connection between the API Server and etcd. Then, verify that apiServerArguments has the etcd-keyfile configured in the openshift-kube-apiserver configmap to something similar to:\n\n...\n\"etcd-keyfile\": [\n   \"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.key\"\n],\n...",
      "id": "xccdf_org.ssgproject.content_rule_api_server_etcd_key",
      "instructions": "Run the following command:\n$ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data[\"config.yaml\"]' | jq -r '.apiServerArguments.\"etcd-keyfile\"'\nThe output should return a configured certificate key file.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "api-server-etcd-key"
        },
        "creationTimestamp": "2024-03-05T15:10:46Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-api-server-etcd-key",
        "namespace": "openshift-compliance",
        "resourceVersion": "82756",
        "uid": "2d644162-4aa8-4035-bee8-6350c8989a67"
      },
      "rationale": "etcd is a highly-available key-value store used by OpenShift deployments for persistent storage of all REST API objects. These objects are sensitive in nature and should be protected by client authentication. This requires the API Server to identify itself to the etcd server using a client certificate and key.",
      "severity": "medium",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Ensure that the --kubelet-https argument is set to true\nThe kube-apiserver ensures https to the kubelet by default. The apiserver flag \"--kubelet-https\" is deprecated and should be either set to \"true\" or omitted from the argument list.",
      "id": "xccdf_org.ssgproject.content_rule_api_server_https_for_kubelet_conn",
      "instructions": "Run the following command:\n$ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data[\"config.yaml\"]' | jq '.apiServerArguments[\"kubelet-https\"]'\nThe output should return true, or no output at all.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "api-server-https-for-kubelet-conn"
        },
        "creationTimestamp": "2024-03-05T15:10:50Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-api-server-https-for-kubelet-conn",
        "namespace": "openshift-compliance",
        "resourceVersion": "82817",
        "uid": "e88d67b7-1895-4ad2-83e8-4ddd5b757a92"
      },
      "rationale": "Connections from the kube-apiserver to kubelets could potentially carry sensitive data such as secrets and keys. It is thus important to use in-transit encryption for any communication between the apiserver and kubelets.",
      "severity": "medium",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Disable Use of the Insecure Bind Address\nOpenShift should not bind to non-loopback insecure addresses. Edit the openshift-kube-apiserver configmap and remove the insecure-bind-address if it exists:\n\n\"apiServerArguments\":{\n ...\n \"insecure-bind-address\":[\n   \"127.0.0.1\"\n ],\n ...",
      "id": "xccdf_org.ssgproject.content_rule_api_server_insecure_bind_address",
      "instructions": "Run the following command:\n$ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data[\"config.yaml\"]' | jq '.apiServerArguments[\"insecure-bind-address\"]'\nThe output should be empty.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "api-server-insecure-bind-address"
        },
        "creationTimestamp": "2024-03-05T15:10:46Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-api-server-insecure-bind-address",
        "namespace": "openshift-compliance",
        "resourceVersion": "82748",
        "uid": "8a0d5fc0-77df-4168-b0b0-61bfe64d8fe2"
      },
      "rationale": "If the API Server is bound to an insecure address the installation would be susceptible to unauthenticated and unencrypted access to the master node(s). The API Server does not perform authentication checking for insecure binds and the traffic is generally not encrypted.",
      "severity": "medium",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Configure the kubelet Certificate Authority for the API Server\nTo ensure OpenShift verifies kubelet certificates before establishing connections, follow the OpenShift documentation and setup the TLS connection between the API Server and kubelets. Edit the openshift-kube-apiserver configmap and set the below parameter if it is not already configured:\n\n\"apiServerArguments\":{\n ...\n \"kubelet-certificate-authority\":\"/etc/kubernetes/static-pod-resources/configmaps/kubelet-serving-ca/ca-bundle.crt\",\n ...",
      "id": "xccdf_org.ssgproject.content_rule_api_server_kubelet_certificate_authority",
      "instructions": "Run the following command:\n$ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data[\"config.yaml\"]' | jq -r '.apiServerArguments.\"kubelet-certificate-authority\"'\nThe output should return a configured certificate.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "api-server-kubelet-certificate-authority"
        },
        "creationTimestamp": "2024-03-05T15:10:50Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "high",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-api-server-kubelet-certificate-authority",
        "namespace": "openshift-compliance",
        "resourceVersion": "82827",
        "uid": "12f5cece-aa21-45b2-bfde-b07bc8cdcdf3"
      },
      "rationale": "Connections from the API Server to the kubelet are used for fetching logs for pods, attaching (through kubectl) to running pods, and using the kubelet port-forwarding functionality. These connections terminate at the kubelet HTTPS endpoint. By default, the API Server does not verify the kubelet serving certificate, which makes the connection subject to man-in-the-middle attacks, and unsafe to run over untrusted and/or public networks.",
      "severity": "high",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Configure the kubelet Certificate File for the API Server\nTo enable certificate based kubelet authentication, edit the config configmap in the openshift-kube-apiserver namespace and set the below parameter in the config.yaml key if it is not already configured:\n\n\"apiServerArguments\":{\n...\n \"kubelet-client-certificate\":\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.crt\",\n...\n}",
      "id": "xccdf_org.ssgproject.content_rule_api_server_kubelet_client_cert",
      "instructions": "Run the following command:\n$ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data[\"config.yaml\"]' | jq '.apiServerArguments[\"kubelet-client-certificate\"]'\nThe output should return /etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.crt\nor /etc/kubernetes/static-pod-certs/secrets/kubelet-client/tls.crt",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "api-server-kubelet-client-cert"
        },
        "creationTimestamp": "2024-03-05T15:10:43Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "high",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-api-server-kubelet-client-cert",
        "namespace": "openshift-compliance",
        "resourceVersion": "82705",
        "uid": "672c5c2c-1a15-4088-9751-f2ad5380b525"
      },
      "rationale": "By default the API Server does not authenticate itself to the kubelet's HTTPS endpoints. Requests from the API Server are treated anonymously. Configuring certificate-based kubelet authentication ensures that the API Server authenticates itself to kubelets when submitting requests.",
      "severity": "high",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Configure the kubelet Certificate Key for the API Server\nTo enable certificate based kubelet authentication, edit the config configmap in the openshift-kube-apiserver namespace and set the below parameter in the config.yaml key if it is not already configured:\n\n\"apiServerArguments\":{\n...\n \"kubelet-client-key\":\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.key\",\n...\n}",
      "id": "xccdf_org.ssgproject.content_rule_api_server_kubelet_client_key",
      "instructions": "Run the following command:\n$ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data[\"config.yaml\"]' | jq '.apiServerArguments[\"kubelet-client-key\"]'\nThe output should return /etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.key\nor /etc/kubernetes/static-pod-certs/secrets/kubelet-client/tls.key",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "api-server-kubelet-client-key"
        },
        "creationTimestamp": "2024-03-05T15:10:51Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "high",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-api-server-kubelet-client-key",
        "namespace": "openshift-compliance",
        "resourceVersion": "82828",
        "uid": "1f750ba8-ac5d-4024-b952-2fd9cc80fa8e"
      },
      "rationale": "By default the API Server does not authenticate itself to the kubelet's HTTPS endpoints. Requests from the API Server are treated anonymously. Configuring certificate-based kubelet authentication ensures that the API Server authenticates itself to kubelets when submitting requests.",
      "severity": "high",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Ensure the openshift-oauth-apiserver service uses TLS\nBy default, the OpenShift OAuth API Server uses TLS. HTTPS should be used for connections between openshift-oauth-apiserver and kube-apiserver. By default, the OpenShift OAuth API Server uses Intermediate profile which requires a minimum TLS version of 1.2.",
      "id": "xccdf_org.ssgproject.content_rule_api_server_oauth_https_serving_cert",
      "instructions": "Run the following command:\n$ oc get APIServer cluster -o yaml\nVerify that the tlsSecurityProfile is not type Old.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "api-server-oauth-https-serving-cert"
        },
        "creationTimestamp": "2024-03-05T15:10:47Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-api-server-oauth-https-serving-cert",
        "namespace": "openshift-compliance",
        "resourceVersion": "82776",
        "uid": "e13b52cc-a808-456f-aed1-388a0798d5f6"
      },
      "rationale": "Connections between the kube-apiserver and the extension openshift-oauth-apiserver could potentially carry sensitive data such as secrets and keys. It is important to use in-transit encryption for any communication between the kube-apiserver and the extension openshift-apiserver.",
      "severity": "medium",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Ensure the openshift-oauth-apiserver service uses TLS\nBy default, the OpenShift API Server uses TLS. HTTPS should be used for connections between openshift-apiserver and kube-apiserver. By default, the OpenShift OAuth API Server uses Intermediate profile which requires a minimum TLS version of 1.2.",
      "id": "xccdf_org.ssgproject.content_rule_api_server_openshift_https_serving_cert",
      "instructions": "Run the following command:\n$ oc get APIServer cluster -o yaml\nVerify that the tlsSecurityProfile is not type Old.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "api-server-openshift-https-serving-cert"
        },
        "creationTimestamp": "2024-03-05T15:10:43Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-api-server-openshift-https-serving-cert",
        "namespace": "openshift-compliance",
        "resourceVersion": "82721",
        "uid": "c141f672-b9be-46ac-8a1f-fe69433e16a1"
      },
      "rationale": "Connections between the kube-apiserver and the extension openshift-apiserver could potentially carry sensitive data such as secrets and keys. It is important to use in-transit encryption for any communication between the kube-apiserver and the extension openshift-apiserver.",
      "severity": "medium",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Profiling is protected by RBAC\nEnsure that the cluster-debugger cluster role includes the /metrics resource URL. This demonstrates that profiling is protected by RBAC, with a specific cluster role to allow access.",
      "id": "xccdf_org.ssgproject.content_rule_api_server_profiling_protected_by_rbac",
      "instructions": "To verify that the cluster-debugger role is configured correctly,\nrun the following command:\n$ oc get clusterroles cluster-debugger -o jsonpath='{.rules[0].nonResourceURLs}'\nand verify that the /metrics path is included there.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "api-server-profiling-protected-by-rbac"
        },
        "creationTimestamp": "2024-03-05T15:10:45Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-api-server-profiling-protected-by-rbac",
        "namespace": "openshift-compliance",
        "resourceVersion": "82738",
        "uid": "93fd9e77-3995-49b4-a397-1c6299f8dafb"
      },
      "rationale": "Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. To ensure the collected data is not exploited, profiling endpoints are secured via RBAC (see cluster-debugger role). By default, the profiling endpoints are accessible only by users bound to cluster-admin or cluster-debugger role. Profiling can not be disabled.",
      "severity": "medium",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Configure the API Server Minimum Request Timeout\nThe API server minimum request timeout defines the minimum number of seconds a handler must keep a request open before timing it out. To set this, edit the openshift-kube-apiserver configmap and set min-request-timeout under the apiServerArguments field:\n\n\"apiServerArguments\":{\n ...\n \"min-request-timeout\":[ 3600 ],\n ...",
      "id": "xccdf_org.ssgproject.content_rule_api_server_request_timeout",
      "instructions": "Run the following command:\n$ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data[\"config.yaml\"]' | jq '.apiServerArguments[\"min-request-timeout\"]'\nThe output should return   .",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "api-server-request-timeout"
        },
        "creationTimestamp": "2024-03-05T15:10:44Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-has-value": "",
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-api-server-request-timeout",
        "namespace": "openshift-compliance",
        "resourceVersion": "82731",
        "uid": "6d74d57d-1d5a-402e-afdb-ccd0605d3720"
      },
      "rationale": "Setting global request timeout allows extending the API Server request timeout limit to a duration appropriate to the user's connection speed. By default, it is set to 1800 seconds which might not be suitable for some environments. Setting the limit too low may result in excessive timeouts, and a limit that is too large may exhaust the API Server resources making it prone to Denial-of-Service attack. It is recommended to set this limit as appropriate and change the default limit of 1800 seconds only if needed.",
      "severity": "medium",
      "status": "PASS",
      "valuesUsed": [
        "var-api-min-request-timeout"
      ]
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Ensure that the service-account-lookup argument is set to true\nValidate service account before validating token.",
      "id": "xccdf_org.ssgproject.content_rule_api_server_service_account_lookup",
      "instructions": "Run the following command:\n$ oc get configmap config -n openshift-kube-apiserver -o json | \\\n    jq -r '.data[\"config.yaml\"]' | \\\n    jq -r '.apiServerArguments[\"service-account-lookup\"]'\nThe output should return true.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "api-server-service-account-lookup"
        },
        "creationTimestamp": "2024-03-05T15:10:48Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-api-server-service-account-lookup",
        "namespace": "openshift-compliance",
        "resourceVersion": "82782",
        "uid": "f0d934f5-b8f7-4d0e-a1a6-923957f18914"
      },
      "rationale": "If service-account-lookup is not enabled, the apiserver only verifies that the authentication token is valid, and does not validate that the service account token mentioned in the request is actually present in etcd. This allows using a service account token even after the corresponding service account is deleted. This is an example of time of check to time of use security issue.",
      "severity": "medium",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Configure the Service Account Public Key for the API Server\nTo ensure the API Server utilizes its own key pair, edit the openshift-kube-apiserver configmap and set the serviceAccountPublicKeyFiles parameter to the public key file for service accounts:\n\n...\n\"serviceAccountPublicKeyFiles\":[\n \"/etc/kubernetes/static-pod-resources/configmaps/sa-token-signing-certs\"\n],\n...",
      "id": "xccdf_org.ssgproject.content_rule_api_server_service_account_public_key",
      "instructions": "Run the following command:\n$ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data[\"config.yaml\"]' | jq -r .serviceAccountPublicKeyFiles\nThe output should return configured certificate key file(s).",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "api-server-service-account-public-key"
        },
        "creationTimestamp": "2024-03-05T15:10:46Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-api-server-service-account-public-key",
        "namespace": "openshift-compliance",
        "resourceVersion": "82751",
        "uid": "f59a8749-f7a9-45ac-affe-e5800c6c3ea8"
      },
      "rationale": "By default if no service-account-key-file is specified to the apiserver, it uses the private key from the TLS serving certificate to verify service account tokens. To ensure that the keys for service account tokens are rotated as needed, a separate public/private key pair should be used for signing service account tokens.",
      "severity": "medium",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Configure the Certificate for the API Server\nTo ensure the API Server utilizes its own TLS certificates, the tls-cert-file must be configured. Verify that the apiServerArguments section has the tls-cert-file configured in the config configmap in the openshift-kube-apiserver namespace similar to:\n\n\"apiServerArguments\":{\n...\n\"tls-cert-file\": [\n \"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\"\n],\n...\n}",
      "id": "xccdf_org.ssgproject.content_rule_api_server_tls_cert",
      "instructions": "Run the following command:\n$ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data[\"config.yaml\"]' | jq -r '.apiServerArguments.\"tls-cert-file\"'\nThe output should return /etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "api-server-tls-cert"
        },
        "creationTimestamp": "2024-03-05T15:10:44Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-api-server-tls-cert",
        "namespace": "openshift-compliance",
        "resourceVersion": "82736",
        "uid": "10259dca-123e-42a0-940e-b992a0075936"
      },
      "rationale": "API Server communication contains sensitive parameters that should remain encrypted in transit. Configure the API Server to serve only HTTPS traffic.",
      "severity": "medium",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Use Strong Cryptographic Ciphers on the API Server\nTo ensure that the API Server is configured to only use strong cryptographic ciphers, verify the openshift-kube-apiserver configmap contains the following set of ciphers, with no additions:\n\n\"servingInfo\":{\n ...\n \"cipherSuites\": [\n   \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\",\n   \"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\n   \"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\",\n   \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\",\n   \"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\",\n   \"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\",\n ],\n ...",
      "id": "xccdf_org.ssgproject.content_rule_api_server_tls_cipher_suites",
      "instructions": "Run the following command:\n$ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data[\"config.yaml\"]' | jq '.servingInfo[\"cipherSuites\"]'\nVerify that the set of ciphers contains only the following:\n\n\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\",\n\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\n\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\",\n\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\",\n\"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\",\n\"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\"",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "api-server-tls-cipher-suites"
        },
        "creationTimestamp": "2024-03-05T15:10:49Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-api-server-tls-cipher-suites",
        "namespace": "openshift-compliance",
        "resourceVersion": "82790",
        "uid": "76a878e7-7e5c-4182-80ee-195594077642"
      },
      "rationale": "TLS ciphers have had a number of known vulnerabilities and weaknesses, which can reduce the protection provided. By default, OpenShift supports a number of TLS ciphersuites including some that have security concerns, weakening the protection provided.",
      "severity": "medium",
      "status": "PASS",
      "warnings": [
        "Once configured, API Server clients that cannot support modern cryptographic ciphers will not be able to make connections to the API server."
      ]
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Configure the Certificate Key for the API Server\nTo ensure the API Server utilizes its own TLS certificates, the tls-private-key-file must be configured. Verify that the apiServerArguments section has the tls-private-key-file configured in the config configmap in the openshift-kube-apiserver namespace similar to:\n\n\"apiServerArguments\":{\n...\n\"tls-private-key-file\": [\n \"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"\n],\n...\n}",
      "id": "xccdf_org.ssgproject.content_rule_api_server_tls_private_key",
      "instructions": "Run the following command:\n$ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data[\"config.yaml\"]' | jq -r '.apiServerArguments.\"tls-private-key-file\"'\nThe output should return /etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "api-server-tls-private-key"
        },
        "creationTimestamp": "2024-03-05T15:10:46Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-api-server-tls-private-key",
        "namespace": "openshift-compliance",
        "resourceVersion": "82753",
        "uid": "506eb441-8ce0-49ea-9f00-f8bd19e8af16"
      },
      "rationale": "API Server communication contains sensitive parameters that should remain encrypted in transit. Configure the API Server to serve only HTTPS traffic.",
      "severity": "medium",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Disable Token-based Authentication\nTo ensure OpenShift does not accept token-based authentication, follow the OpenShift documentation and configure alternate mechanisms for authentication. Then, edit the API Server pod specification file Edit the openshift-kube-apiserver configmap and remove the token-auth-file parameter:\n\n\"apiServerArguments\":{\n ...\n \"token-auth-file\":[\n   \"/path/to/any/file\"\n ],\n ...",
      "id": "xccdf_org.ssgproject.content_rule_api_server_token_auth",
      "instructions": "Run the following command:\n$ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data[\"config.yaml\"]' | jq '.apiServerArguments' | grep \"token-auth-file\"\nThe output should be empty as OpenShift does not support token-based authentication at all.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "api-server-token-auth"
        },
        "creationTimestamp": "2024-03-05T15:10:45Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "high",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-api-server-token-auth",
        "namespace": "openshift-compliance",
        "resourceVersion": "82739",
        "uid": "9c062036-cc60-49cc-bbab-62a4393f9a5d"
      },
      "rationale": "The token-based authentication utilizes static tokens to authenticate requests to the API Server. The tokens are stored in clear-text in a file on the API Server, and cannot be revoked or rotated without restarting the API Server.",
      "severity": "high",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Ensure that Audit Log Forwarding Is Enabled\nOpenShift audit works at the API server level, logging all requests coming to the server. Audit is on by default and the best practice is to ship audit logs off the cluster for retention. The cluster-logging-operator is able to do this with the\n\nClusterLogForwarders\n\nresource. The forementioned resource can be configured to logs to different third party systems. For more information on this, please reference the official documentation: https://docs.openshift.com/container-platform/4.6/logging/cluster-logging-external.html",
      "id": "xccdf_org.ssgproject.content_rule_audit_log_forwarding_enabled",
      "instructions": "Run the following command:\noc get clusterlogforwarders instance -n openshift-logging -ojson | jq -r '.spec.pipelines[].inputRefs | contains([\"audit\"])'\nThe output should return true.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "audit-log-forwarding-enabled"
        },
        "creationTimestamp": "2024-03-05T15:10:49Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "FAIL",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-audit-log-forwarding-enabled",
        "namespace": "openshift-compliance",
        "resourceVersion": "82789",
        "uid": "e2fe2817-141e-48b0-ac25-230aca31981d"
      },
      "rationale": "Retaining logs ensures the ability to go back in time to investigate or correlate any events. Offloading audit logs from the cluster ensures that an attacker that has access to the cluster will not be able to tamper with the logs because of the logs being stored off-site.",
      "severity": "medium",
      "status": "FAIL"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Ensure that the cluster's audit profile is properly set\nOpenShift can audit the details of requests made to the API server through the standard Kubernetes audit capabilities.\n\nIn OpenShift, auditing of the API Server is on by default. Audit provides a security-relevant chronological set of records documenting the sequence of activities that have affected system by individual users, administrators, or other components of the system. Audit works at the API server level, logging all requests coming to the server. Each audit log contains two entries:\n\nThe request line containing:\n\n* A Unique ID allowing to match the response line (see #2)\n* The source IP of the request\n* The HTTP method being invoked\n* The original user invoking the operation\n* The impersonated user for the operation (self meaning himself)\n* The impersonated group for the operation (lookup meaning user's group)\n* The namespace of the request or none\n* The URI as requested\n\nThe response line containing:\n\n* The aforementioned unique ID\n* The response code\n\nFor more information on how to configure the audit profile, please visit the documentation ( https://docs.openshift.com/container-platform/4.6/security/audit-log-policy-config.html )",
      "id": "xccdf_org.ssgproject.content_rule_audit_profile_set",
      "instructions": "Run the following command to retrieve the current audit profile:\n$ oc get apiservers cluster -ojsonpath='{.spec.audit.profile}'\nMake sure the profile returned matches the one that should be used.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "audit-profile-set"
        },
        "creationTimestamp": "2024-03-05T15:10:50Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/automated-remediation": "",
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "FAIL",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-audit-profile-set",
        "namespace": "openshift-compliance",
        "resourceVersion": "82821",
        "uid": "ded75d1d-0b0a-4aba-b24f-eb77a7634e8f"
      },
      "rationale": "Logging is an important detective control for all systems, to detect potential unauthorised access.",
      "severity": "medium",
      "status": "FAIL"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Ensure that the CNI in use supports Network Policies\nThere are a variety of CNI plugins available for Kubernetes. If the CNI in use does not support Network Policies it may not be possible to effectively restrict traffic in the cluster. OpenShift supports Kubernetes NetworkPolicy using a Kubernetes Container Network Interface (CNI) plug-in.",
      "id": "xccdf_org.ssgproject.content_rule_configure_network_policies",
      "instructions": "Verify that your CNI plugin supports NetworkPolicies:\n$ oc get network cluster -ojsonpath='{.spec.networkType}'\nThe result should list a CNI plugin that supports NetworkPolicies,\ncurrently the plugins in the rule's pass list are OpenShiftSDN, OVN\nand Calico.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "configure-network-policies"
        },
        "creationTimestamp": "2024-03-05T15:10:50Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "high",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-configure-network-policies",
        "namespace": "openshift-compliance",
        "resourceVersion": "82825",
        "uid": "a31d2a3a-6ccb-443c-a984-10042a6907ed"
      },
      "rationale": "Kubernetes network policies are enforced by the CNI plugin in use. As such it is important to ensure that the CNI plugin supports both Ingress and Egress network policies.",
      "severity": "high",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Ensure that application Namespaces have Network Policies defined.\nUse network policies to isolate traffic in your cluster network.",
      "id": "xccdf_org.ssgproject.content_rule_configure_network_policies_namespaces",
      "instructions": "Verify that the every non-control plane namespace has an appropriate\nNetworkPolicy.\n\nTo get all the non-control plane namespaces, you can do the\nfollowing command oc get  namespaces -o json | jq '[.items[] | select((.metadata.name | startswith(\"openshift\") | not) and (.metadata.name | startswith(\"kube-\") | not) and .metadata.name != \"default\") | .metadata.name ]'\n\nTo get all the non-control plane namespaces with a NetworkPolicy, you can do the\nfollowing command oc get --all-namespaces networkpolicies -o json | jq '[.items[] | select((.metadata.namespace | startswith(\"openshift\") | not) and (.metadata.namespace | startswith(\"kube-\") | not) and .metadata.namespace != \"default\") | .metadata.namespace] | unique'\n\nMake sure that the namespaces displayed in the commands of the commands match.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "configure-network-policies-namespaces"
        },
        "creationTimestamp": "2024-03-05T15:10:48Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "high",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-configure-network-policies-namespaces",
        "namespace": "openshift-compliance",
        "resourceVersion": "82783",
        "uid": "fc06aefc-60af-4848-a0cc-199c92c01736"
      },
      "rationale": "Running different applications on the same Kubernetes cluster creates a risk of one compromised application attacking a neighboring application. Network segmentation is important to ensure that containers can communicate only with those they are supposed to. When a network policy is introduced to a given namespace, all traffic not allowed by the policy is denied. However, if there are no network policies in a namespace all traffic will be allowed into and out of the pods in that namespace.",
      "severity": "high",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Ensure Controller insecure port argument is unset\nTo ensure the Controller Manager service is bound to secure loopback address and a secure port, set the RotateKubeletServerCertificate option to true in the openshift-kube-controller-manager configmap on the master node(s):\n\n\"extendedArguments\": {\n...\n \"port\": [\"0\"],\n...\n\nIt is also acceptable for a system to deprecate the insecure port:\n\n\"extendedArguments\": {\n...\n...",
      "id": "xccdf_org.ssgproject.content_rule_controller_insecure_port_disabled",
      "instructions": "To verify that port is configured correctly,\nrun the following command:\n$ oc get configmaps config -n openshift-kube-controller-manager -ojson | jq ''\nVerify that it's true in the console output (the value will be true if the insecure port is bind to loopback address or disabled) .",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "controller-insecure-port-disabled"
        },
        "creationTimestamp": "2024-03-05T15:10:43Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "low",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-controller-insecure-port-disabled",
        "namespace": "openshift-compliance",
        "resourceVersion": "82722",
        "uid": "d48751bb-6368-4217-932c-c117e57e15f1"
      },
      "rationale": "The Controller Manager API service is used for health and metrics information and is available without authentication or encryption. As such, it should only be bound to a localhost interface to minimize the cluster's attack surface.",
      "severity": "low",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Ensure Controller secure-port argument is set\nTo ensure the Controller Manager service is bound to secure loopback address using a secure port, set the RotateKubeletServerCertificate option to true in the openshift-kube-controller-manager configmap on the master node(s):\n\n\"extendedArguments\": {\n...\n \"secure-port\": [\"10257\"],\n...",
      "id": "xccdf_org.ssgproject.content_rule_controller_secure_port",
      "instructions": "To verify that secure-port is configured correctly,\nrun the following command:\n$ oc get configmaps config -n openshift-kube-controller-manager -ojson |   jq -r '.data[\"config.yaml\"]' | jq '.extendedArguments[\"secure-port\"][]'\nVerify that it's using an appropriate port (the value is not 0).",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "controller-secure-port"
        },
        "creationTimestamp": "2024-03-05T15:10:47Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "low",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-controller-secure-port",
        "namespace": "openshift-compliance",
        "resourceVersion": "82765",
        "uid": "324d1b80-7dd3-4bfa-bcfb-f610b4354a7d"
      },
      "rationale": "The Controller Manager API service is used for health and metrics information and is available without authentication or encryption. As such, it should only be bound to a localhost interface to minimize the cluster's attack surface.",
      "severity": "low",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Configure the Service Account Certificate Authority Key for the Controller Manager\nTo ensure the API Server utilizes its own key pair, set the masterCA parameter to the public key file for service accounts in the openshift-kube-controller-manager configmap on the master node(s):\n\n\"extendedArguments\": {\n...\n \"root-ca-file\": [\n   \"/etc/kubernetes/static-pod-resources/configmaps/serviceaccount-ca/ca-bundle.crt\"\n ],\n...",
      "id": "xccdf_org.ssgproject.content_rule_controller_service_account_ca",
      "instructions": "To verify that root-ca-file is configured correctly,\nrun the following command:\n$ oc get configmaps config -n openshift-kube-controller-manager -ojson | jq -r '.data[\"config.yaml\"]' | jq -r '.extendedArguments[\"root-ca-file\"]'\nThe output should return a configured certificate authority file.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "controller-service-account-ca"
        },
        "creationTimestamp": "2024-03-05T15:10:44Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-controller-service-account-ca",
        "namespace": "openshift-compliance",
        "resourceVersion": "82737",
        "uid": "1d758338-4719-40cf-a016-998bb16039ad"
      },
      "rationale": "Service accounts authenticate to the API using tokens signed by a private RSA key. The authentication layer verifies the signature using a matching public RSA key. Configuring the certificate authority file ensures that the API server's signing certificates are validated.",
      "severity": "medium",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Configure the Service Account Private Key for the Controller Manager\nTo ensure the API Server utilizes its own key pair, set the privateKeyFile parameter to the public key file for service accounts in the openshift-kube-controller-manager configmap on the master node(s):\n\n\"extendedArguments\": {\n...\n \"service-account-private-key-file\": [\n   \"/etc/kubernetes/static-pod-resources/secrets/service-account-private-key/service-account.key\"\n ],\n...",
      "id": "xccdf_org.ssgproject.content_rule_controller_service_account_private_key",
      "instructions": "To verify that service-account-private-key-file is configured correctly,\nrun the following command:\n$ oc get configmaps config -n openshift-kube-controller-manager -ojson | jq -r '.data[\"config.yaml\"]' | jq -r '.extendedArguments[\"service-account-private-key-file\"]'\nThe output should return a configured private key file.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "controller-service-account-private-key"
        },
        "creationTimestamp": "2024-03-05T15:10:51Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-controller-service-account-private-key",
        "namespace": "openshift-compliance",
        "resourceVersion": "82829",
        "uid": "8ac85cfa-0185-4a9a-ad81-b71c5e7fc19a"
      },
      "rationale": "By default if no private key file is specified to the API Server, the API Server uses the private key from the TLS serving certificate to verify service account tokens. To ensure that the keys for service account tokens could be rotated as needed, a separate public/private key pair should be used for signing service account tokens.",
      "severity": "medium",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Ensure that use-service-account-credentials is enabled\nTo ensure individual service account credentials are used, set the use-service-account-credentials option to true in the openshift-kube-controller-manager configmap on the master node(s):\n\n\"extendedArguments\": {\n...\n \"use-service-account-credentials\": [\n   \"true\"\n ],\n...",
      "id": "xccdf_org.ssgproject.content_rule_controller_use_service_account",
      "instructions": "To verify that service-account-credentials is configured correctly,\nrun the following command:\n$ oc get configmaps config -n openshift-kube-controller-manager -ojson | jq -r '.data[\"config.yaml\"]' | jq -r '.extendedArguments[\"use-service-account-credentials\"]'\nThe value of use-service-account-credentials should be true.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "controller-use-service-account"
        },
        "creationTimestamp": "2024-03-05T15:10:50Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-controller-use-service-account",
        "namespace": "openshift-compliance",
        "resourceVersion": "82815",
        "uid": "5094ca60-e266-4280-aba0-2cf04435c8f4"
      },
      "rationale": "The controller manager creates a service account per controller in kube-system namespace, generates an API token and credentials for it, then builds a dedicated API client with that service account credential for each controller loop to use. Setting the use-service-account-credentials to true runs each control loop within the controller manager using a separate service account credential. When used in combination with RBAC, this ensures that the control loops run with the minimum permissions required to perform their intended tasks.",
      "severity": "medium",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Disable etcd Self-Signed Certificates\nTo ensure the etcd service is not using self-signed certificates, run the following command:\n\n$ oc get cm/etcd-pod -n openshift-etcd -o yaml\n\nThe etcd pod configuration contained in the configmap should not contain the --auto-tls=true flag.",
      "id": "xccdf_org.ssgproject.content_rule_etcd_auto_tls",
      "instructions": "Run the following command:\n$ oc get cm/etcd-pod -n openshift-etcd -o yaml\nThe etcd pod configuration contained in the configmap should not\ncontain the --auto-tls=true flag.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "etcd-auto-tls"
        },
        "creationTimestamp": "2024-03-05T15:10:44Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-etcd-auto-tls",
        "namespace": "openshift-compliance",
        "resourceVersion": "82723",
        "uid": "540b2362-e532-4d34-8e07-b9e41d8300c8"
      },
      "rationale": "Without cryptographic integrity protections, information can be altered by unauthorized users without detection. Using self-signed certificates ensures that the certificates are never validated against a certificate authority and could lead to compromised and invalidated data.",
      "severity": "medium",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Ensure That The etcd Client Certificate Is Correctly Set\nTo ensure the etcd service is serving TLS to clients, make sure the etcd-pod* ConfigMaps in the openshift-etcd namespace contain the following argument for the etcd binary in the etcd pod:\n\n--cert-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-[a-z]+/etcd-serving-NODE_NAME.crt\n\n. Note that the\n\n[a-z]+\n\nis being used since the directory might change between OpenShift versions.",
      "id": "xccdf_org.ssgproject.content_rule_etcd_cert_file",
      "instructions": "Run the following command:\noc get -nopenshift-etcd cm etcd-pod -oyaml | grep -E \"\\-\\-cert-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-[a-z]+/etcd-serving-NODE_NAME.crt\"\nVerify that there is a certificate configured.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "etcd-cert-file"
        },
        "creationTimestamp": "2024-03-05T15:10:43Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-etcd-cert-file",
        "namespace": "openshift-compliance",
        "resourceVersion": "82718",
        "uid": "ed915267-1d85-4914-83d4-3d1d2db0a3fc"
      },
      "rationale": "Without cryptographic integrity protections, information can be altered by unauthorized users without detection.",
      "severity": "medium",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Enable The Client Certificate Authentication\nTo ensure the etcd service is serving TLS to clients, make sure the etcd-pod* ConfigMaps in the openshift-etcd namespace contain the following argument for the etcd binary in the etcd pod:\n\noc get -nopenshift-etcd cm etcd-pod -oyaml | grep \"\\-\\-client-cert-auth=\"\n\nthe parameter should be set to true.",
      "id": "xccdf_org.ssgproject.content_rule_etcd_client_cert_auth",
      "instructions": "Run the following command:\noc get -nopenshift-etcd cm etcd-pod -oyaml | grep \"\\-\\-client-cert-auth=\"\nThe parameter should be set to true.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "etcd-client-cert-auth"
        },
        "creationTimestamp": "2024-03-05T15:10:43Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-etcd-client-cert-auth",
        "namespace": "openshift-compliance",
        "resourceVersion": "82719",
        "uid": "931bec9f-ce6c-4944-b163-c94186c98536"
      },
      "rationale": "Without cryptographic integrity protections, information can be altered by unauthorized users without detection.",
      "severity": "medium",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Ensure That The etcd Key File Is Correctly Set\nTo ensure the etcd service is serving TLS to clients, make sure the etcd-pod* ConfigMaps in the openshift-etcd namespace contain the following argument for the etcd binary in the etcd pod:\n\noc get -nopenshift-etcd cm etcd-pod -oyaml | grep \"\\-\\-key-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-[a-z]+/etcd-serving-NODE_NAME.key\"\n\n. Note that the\n\n[a-z]+\n\nis being used since the directory might change between OpenShift versions.",
      "id": "xccdf_org.ssgproject.content_rule_etcd_key_file",
      "instructions": "Run the following command:\noc get -nopenshift-etcd cm etcd-pod -oyaml | grep \"\\-\\-key-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-[a-z]+/etcd-serving-NODE_NAME.key\"\nVerify that there is a private key configured.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "etcd-key-file"
        },
        "creationTimestamp": "2024-03-05T15:10:46Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-etcd-key-file",
        "namespace": "openshift-compliance",
        "resourceVersion": "82754",
        "uid": "504382ea-9a64-463f-b1f5-5235b2fbf23a"
      },
      "rationale": "Without cryptographic integrity protections, information can be altered by unauthorized users without detection.",
      "severity": "medium",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Disable etcd Peer Self-Signed Certificates\nTo ensure the etcd service is not using self-signed certificates, run the following command:\n\n$ oc get cm/etcd-pod -n openshift-etcd -o yaml\n\nThe etcd pod configuration contained in the configmap should not contain the --peer-auto-tls=true flag.",
      "id": "xccdf_org.ssgproject.content_rule_etcd_peer_auto_tls",
      "instructions": "Run the following command:\n$ oc get cm/etcd-pod -n openshift-etcd -o yaml\nThe etcd pod configuration contained in the configmap should not\ncontain the --peer-auto-tls=true flag.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "etcd-peer-auto-tls"
        },
        "creationTimestamp": "2024-03-05T15:10:49Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-etcd-peer-auto-tls",
        "namespace": "openshift-compliance",
        "resourceVersion": "82811",
        "uid": "b792727d-a311-4780-8252-3503e8d52687"
      },
      "rationale": "Without cryptographic integrity protections, information can be altered by unauthorized users without detection. Using self-signed certificates ensures that the certificates are never validated against a certificate authority and could lead to compromised and invalidated data.",
      "severity": "medium",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Ensure That The etcd Peer Client Certificate Is Correctly Set\nTo ensure the etcd service is serving TLS to peers, make sure the etcd-pod* ConfigMaps in the openshift-etcd namespace contain the following argument for the etcd binary in the etcd pod:\n\n--peer-cert-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-[a-z]+/etcd-peer-NODE_NAME.crt\n\nNote that the\n\n[a-z]+\n\nis being used since the directory might change between OpenShift versions.",
      "id": "xccdf_org.ssgproject.content_rule_etcd_peer_cert_file",
      "instructions": "Run the following command:\noc get -nopenshift-etcd cm etcd-pod -oyaml | grep \"\\-\\-peer-cert-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-[a-z]+/etcd-peer-NODE_NAME.crt\"\nVerify that there is a certificate configured.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "etcd-peer-cert-file"
        },
        "creationTimestamp": "2024-03-05T15:10:43Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-etcd-peer-cert-file",
        "namespace": "openshift-compliance",
        "resourceVersion": "82706",
        "uid": "a61a113c-84a1-4114-b6b8-48182f2ab13f"
      },
      "rationale": "Without cryptographic integrity protections, information can be altered by unauthorized users without detection.",
      "severity": "medium",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Enable The Peer Client Certificate Authentication\nTo ensure the etcd service is serving TLS to clients, make sure the etcd-pod* ConfigMaps in the openshift-etcd namespace contain the following argument for the etcd binary in the etcd pod:\n\noc get -nopenshift-etcd cm etcd-pod -oyaml | grep \"\\-\\-peer-client-cert-auth=\"\n\nthe parameter should be set to true.",
      "id": "xccdf_org.ssgproject.content_rule_etcd_peer_client_cert_auth",
      "instructions": "Run the following command:\noc get -nopenshift-etcd cm etcd-pod -oyaml | grep \"\\-\\-peer-client-cert-auth=\"\nThe parameter should be set to true.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "etcd-peer-client-cert-auth"
        },
        "creationTimestamp": "2024-03-05T15:10:47Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-etcd-peer-client-cert-auth",
        "namespace": "openshift-compliance",
        "resourceVersion": "82775",
        "uid": "9b81a84b-aad6-479e-bf59-17d435d50620"
      },
      "rationale": "Without cryptographic integrity protections, information can be altered by unauthorized users without detection.",
      "severity": "medium",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Ensure That The etcd Peer Key File Is Correctly Set\nTo ensure the etcd service is serving TLS to peers, make sure the etcd-pod* ConfigMaps in the openshift-etcd namespace contain the following argument for the etcd binary in the etcd pod:\n\noc get -nopenshift-etcd cm etcd-pod -oyaml | grep \"\\-\\-peer-key-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-[a-z]+/etcd-peer-NODE_NAME.key\"\n\nNote that the\n\n[a-z]+\n\nis being used since the directory might change between OpenShift versions.",
      "id": "xccdf_org.ssgproject.content_rule_etcd_peer_key_file",
      "instructions": "Run the following command:\noc get -nopenshift-etcd cm etcd-pod -oyaml | grep \"\\-\\-peer-key-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-[a-z]+/etcd-peer-NODE_NAME.key\"\nVerify that there is a private key configured.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "etcd-peer-key-file"
        },
        "creationTimestamp": "2024-03-05T15:10:45Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-etcd-peer-key-file",
        "namespace": "openshift-compliance",
        "resourceVersion": "82745",
        "uid": "725c2cc6-cdd9-47d1-bec0-a9927e0b468e"
      },
      "rationale": "Without cryptographic integrity protections, information can be altered by unauthorized users without detection.",
      "severity": "medium",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Apply Security Context to Your Pods and Containers\nApply Security Context to your Pods and Containers",
      "id": "xccdf_org.ssgproject.content_rule_general_apply_scc",
      "instructions": "Review the pod definitions in your cluster and verify they have appropriate\nsecurity contexts. OpenShift comes configured with default security context\nconstraints you can use immediately to secure pods in your cluster. For more\ninformation on security context constraints, how to use them, and how to\nbuild your own, please refer to the OpenShift security constraints documentation.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "general-apply-scc"
        },
        "creationTimestamp": "2024-03-05T15:10:43Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "MANUAL",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-general-apply-scc",
        "namespace": "openshift-compliance",
        "resourceVersion": "82708",
        "uid": "df94ff9a-5358-489d-8a37-f9b21e169a32"
      },
      "rationale": "A security context defines the operating system security settings (uid, gid, capabilities, SELinux role, etc..) applied to a container. When designing your containers and pods, make sure that you configure the security context for your pods, containers, and volumes. A security context is a property defined in the deployment yaml. It controls the security parameters that will be assigned to the pod/container/volume. There are two levels of security context: pod level security context, and container level security context.",
      "severity": "medium",
      "status": "MANUAL"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "The default namespace should not be used\nKubernetes provides a default namespace, where objects are placed if no namespace is specified for them. Placing objects in this namespace makes application of RBAC and other controls more difficult.",
      "id": "xccdf_org.ssgproject.content_rule_general_default_namespace_use",
      "instructions": "Run the following command to list objects in the default namespace:\n$ oc get all -n default\nThe only entries there should be system-managed resources such as the\nkubernetes and openshift service.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "general-default-namespace-use"
        },
        "creationTimestamp": "2024-03-05T15:10:45Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "MANUAL",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-general-default-namespace-use",
        "namespace": "openshift-compliance",
        "resourceVersion": "82747",
        "uid": "00c5ee79-3312-4b53-a70c-27f96db65cd5"
      },
      "rationale": "Resources in a Kubernetes cluster should be segregated by namespace, to allow for security controls to be applied at that level and to make it easier to manage resources.",
      "severity": "medium",
      "status": "MANUAL"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Ensure Seccomp Profile Pod Definitions\nEnable default seccomp profiles in your pod definitions.",
      "id": "xccdf_org.ssgproject.content_rule_general_default_seccomp_profile",
      "instructions": "In OpenShift 4, CRI-O is the supported runtime. CRI-O runs unconfined by\ndefault in order to meet CRI conformance criteria.  On RHEL CoreOS, the\ndefault seccomp policy is associated with CRI-O and stored in\n/etc/crio/seccomp.json.  The default profile is applied when the user asks\nfor the runtime/default profile via annotation to the pod and when the\nassociated SCC allows use of the specified seccomp profile.\n\nConfiguration of allowable seccomp profiles is managed through OpenShift\nSecurity Context Constraints.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "general-default-seccomp-profile"
        },
        "creationTimestamp": "2024-03-05T15:10:47Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "MANUAL",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-general-default-seccomp-profile",
        "namespace": "openshift-compliance",
        "resourceVersion": "82768",
        "uid": "849d8b65-6ca7-4589-89f0-360565cb8ff1"
      },
      "rationale": "Seccomp (secure computing mode) is used to restrict the set of system calls applications can make, allowing cluster administrators greater control over the security of workloads running in the cluster. Kubernetes disables seccomp profiles by default for historical reasons. You should enable it to ensure that the workloads have restricted actions available within the container.",
      "severity": "medium",
      "status": "MANUAL"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Create administrative boundaries between resources using namespaces\nUse namespaces to isolate your Kubernetes objects.",
      "id": "xccdf_org.ssgproject.content_rule_general_namespaces_in_use",
      "instructions": "OpenShift projects wrap Kubernetes namespaces and are used by default in\nOpenShift 4.  Run the following command and review the namespaces created in\nthe cluster.  $ oc get namespaces Ensure that the namespaces are\nthe ones you need and are adequately administered.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "general-namespaces-in-use"
        },
        "creationTimestamp": "2024-03-05T15:10:46Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "MANUAL",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-general-namespaces-in-use",
        "namespace": "openshift-compliance",
        "resourceVersion": "82750",
        "uid": "70867aae-7a7f-4ed9-a11a-3b451c4af3f6"
      },
      "rationale": "Limiting the scope of user permissions can reduce the impact of mistakes or malicious activities. A Kubernetes namespace allows you to partition created resources into logically named groups. Resources created in one namespace can be hidden from other namespaces. By default, each resource created by a user in Kubernetes cluster runs in a default namespace, called default. You can create additional namespaces and attach resources and users to them. You can use Kubernetes Authorization plugins to create policies that segregate access to namespace resources between different users.",
      "severity": "medium",
      "status": "MANUAL"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Configure An Identity Provider\nFor users to interact with OpenShift Container Platform, they must first authenticate to the cluster. The authentication layer identifies the user associated with requests to the OpenShift Container Platform API. The authorization layer then uses information about the requesting user to determine if the request is allowed. Understanding authentication | Authentication | OpenShift Container Platform ( https://docs.openshift.com/container-platform/4.6/logging/cluster-logging-external.html )\n\nThe OpenShift Container Platform includes a built-in OAuth server for token-based authentication. Developers and administrators obtain OAuth access tokens to authenticate themselves to the API. It is recommended for an administrator to configure OAuth to specify an identity provider after the cluster is installed. User access to the cluster is managed through the identity provider. Understanding identity provider configuration | Authentication | OpenShift Container Platform ( https://docs.openshift.com/container-platform/4.6/authentication/understanding-identity-provider.html )\n\nOpenShift includes built-in role based access control (RBAC) to determine whether a user is allowed to perform a given action within the cluster. Roles can have cluster scope or local (i.e. project) scope. Using RBAC to define and apply permissions | Authentication | OpenShift Container Platform ( https://docs.openshift.com/container-platform/4.6/authentication/using-rbac.html )",
      "id": "xccdf_org.ssgproject.content_rule_idp_is_configured",
      "instructions": "Run the following command to list the identity providers configured:\n$ oc get oauths cluster -ojsonpath='{.spec.identityProviders}' | jq \nMake sure that there exists at least one item referenced by the above path.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "idp-is-configured"
        },
        "creationTimestamp": "2024-03-05T15:10:43Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "FAIL",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-idp-is-configured",
        "namespace": "openshift-compliance",
        "resourceVersion": "82714",
        "uid": "bf8c7ac3-0977-4154-a718-55f4fe2a8e1d"
      },
      "rationale": "With any authentication mechanism the ability to revoke credentials if they are compromised or no longer required, is a key control. Kubernetes client certificate authentication does not allow for this due to a lack of support for certificate revocation.\n\nOpenShift's built-in OAuth server allows credential revocation by relying on the Identity provider, as well as giving the administrators the ability to revoke any tokens given to a specific user.",
      "severity": "medium",
      "status": "FAIL"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Ensure that the kubeadmin secret has been removed\nThe kubeadmin user is meant to be a temporary user used for bootstrapping purposes. It is preferable to assign system administrators whose users are backed by an Identity Provider.\n\nMake sure to remove the user as described in the documentation ( https://docs.openshift.com/container-platform/latest/authentication/remove-kubeadmin.html )",
      "id": "xccdf_org.ssgproject.content_rule_kubeadmin_removed",
      "instructions": "To verify that the kubeadmin secret has been deleted, make sure\nthat oc get secrets kubeadmin -n kube-system\nreturns a NotFound error.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "kubeadmin-removed"
        },
        "creationTimestamp": "2024-03-05T15:10:49Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "FAIL",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-kubeadmin-removed",
        "namespace": "openshift-compliance",
        "resourceVersion": "82793",
        "uid": "cf68b428-4241-4b93-bcf7-299ef605799d"
      },
      "rationale": "The kubeadmin user has an auto-generated password and a self-signed certificate, and has effectively\n\ncluster-admin\n\npermissions; therefore, it's considered a security liability.",
      "severity": "medium",
      "status": "FAIL"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Ensure That The kubelet Client Certificate Is Correctly Set\nTo ensure the kubelet TLS client certificate is configured, edit the kubelet configuration file /etc/kubernetes/kubelet.conf and configure the kubelet certificate file.\n\ntlsCertFile: /path/to/TLS/cert.key",
      "id": "xccdf_org.ssgproject.content_rule_kubelet_configure_tls_cert",
      "instructions": "Run the following command on the kubelet node(s):\n$ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data[\"config.yaml\"]' | jq -r '.apiServerArguments[\"kubelet-client-certificate\"]'\nVerify that a client certificate is configured.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "kubelet-configure-tls-cert"
        },
        "creationTimestamp": "2024-03-05T15:10:49Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-kubelet-configure-tls-cert",
        "namespace": "openshift-compliance",
        "resourceVersion": "82802",
        "uid": "aa4c8541-907f-4eb0-ac3f-1406ba3e2a03"
      },
      "rationale": "Without cryptographic integrity protections, information can be altered by unauthorized users without detection.",
      "severity": "medium",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Ensure That The kubelet Server Key Is Correctly Set\nTo ensure the kubelet TLS private server key certificate is configured, edit the kubelet configuration file /etc/kubernetes/kubelet.conf and configure the kubelet private key file.\n\ntlsPrivateKeyFile: /path/to/TLS/private.key",
      "id": "xccdf_org.ssgproject.content_rule_kubelet_configure_tls_key",
      "instructions": "Run the following command on the kubelet node(s):\n$ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data[\"config.yaml\"]' | jq -r '.apiServerArguments[\"kubelet-client-key\"]'\nVerify that a client certificate is configured.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "kubelet-configure-tls-key"
        },
        "creationTimestamp": "2024-03-05T15:10:43Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-kubelet-configure-tls-key",
        "namespace": "openshift-compliance",
        "resourceVersion": "82710",
        "uid": "a5ab4137-b02e-4e06-aac8-b9a8ab8325c8"
      },
      "rationale": "Without cryptographic integrity protections, information can be altered by unauthorized users without detection.",
      "severity": "medium",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "kubelet - Disable the Read-Only Port\nTo disable the read-only port, edit the kubelet configuration Edit the openshift-kube-apiserver configmap and set the kubelet-read-only-port parameter to 0:\n\n\"apiServerArguments\":{\n ...\n \"kubelet-read-only-port\":[\n   \"0\"\n ],\n ...",
      "id": "xccdf_org.ssgproject.content_rule_kubelet_disable_readonly_port",
      "instructions": "Run the following command:\n$ oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data[\"config.yaml\"]' | jq -r '.apiServerArguments[\"kubelet-read-only-port\"]'\nThe output should be 0.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "kubelet-disable-readonly-port"
        },
        "creationTimestamp": "2024-03-05T15:10:47Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-kubelet-disable-readonly-port",
        "namespace": "openshift-compliance",
        "resourceVersion": "82777",
        "uid": "04a545ab-0b34-4dcc-9670-28af20cff407"
      },
      "rationale": "OpenShift disables the read-only port ( 10255 ) on all nodes by setting the read-only port kubelet flag to 0. This ensures only authenticated connections are able to receive information about the OpenShift system.",
      "severity": "medium",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Allowed registries are configured\nThe configuration registrySources.allowedRegistries determines the permitted registries that the OpenShift container runtime can access for builds and pods. This configuration setting ensures that all registries other than those specified are blocked. You can set the allowed repositories by applying the following manifest using\n\noc patch\n\n, e.g. if you save the following snippet to\n\n/tmp/allowed-registries-patch.yaml\n\nspec:\n registrySources:\n   allowedRegistries:\n   - my-trusted-registry.internal.example.com\n\nyou would call\n\noc patch image.config.openshift.io cluster --patch=\"$(cat /tmp/allowed-registries-patch.yaml)\" --type=merge",
      "id": "xccdf_org.ssgproject.content_rule_ocp_allowed_registries",
      "instructions": "To verify that the allowed registries are configured, run the following:\n$ oc get image.config.openshift.io cluster -ojsonpath='{.spec.registrySources.allowedRegistries}'\nmake sure the output is not empty and matches the registries that you wish to allow.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "ocp-allowed-registries"
        },
        "creationTimestamp": "2024-03-05T15:10:43Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "FAIL",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-ocp-allowed-registries",
        "namespace": "openshift-compliance",
        "resourceVersion": "82704",
        "uid": "9e0584a3-9c7d-4dd2-8f5d-78177dd2530e"
      },
      "rationale": "Allowed registries should be configured to restrict the registries that the OpenShift container runtime can access, and all other registries should be blocked.",
      "severity": "medium",
      "status": "FAIL"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Allowed registries for import are configured\nThe configuration allowedRegistriesForImport limits the container image registries from which normal users may import images. This is important to control, as a user who can stand up a malicious registry can then import content which claims to include the SHAs of legitimate content layers. You can set the allowed repositories for import by applying the following manifest using\n\noc patch\n\n, e.g. if you save the following snippet to\n\n/tmp/allowed-import-registries-patch.yaml\n\nspec:\n allowedRegistriesForImport:\n - domainName: my-trusted-registry.internal.example.com\n   insecure: false\n\nyou would call\n\noc patch image.config.openshift.io cluster --patch=\"$(cat /tmp/allowed-import-registries-patch.yaml)\" --type=merge",
      "id": "xccdf_org.ssgproject.content_rule_ocp_allowed_registries_for_import",
      "instructions": "To verify that the allowed registries for import are configured, run the following:\n$ oc get image.config.openshift.io cluster -ojsonpath='{.spec.allowedRegistriesForImport[:].domainName}'\nmake sure the output is not empty and matches the registries that you wish to allow\nimporting from.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "ocp-allowed-registries-for-import"
        },
        "creationTimestamp": "2024-03-05T15:10:47Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "FAIL",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-ocp-allowed-registries-for-import",
        "namespace": "openshift-compliance",
        "resourceVersion": "82773",
        "uid": "06d80ca7-a1b7-4ab7-856b-9b3d5a87f959"
      },
      "rationale": "Allowed registries for import should be specified to limit the registries from which users may import images.",
      "severity": "medium",
      "status": "FAIL"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Configure the OpenShift API Server Maximum Retained Audit Logs\nTo configure how many rotations of audit logs are retained, edit the openshift-apiserver configmap and set the audit-log-maxbackup parameter to 10 or to an organizationally appropriate value:\n\n\"apiServerArguments\":{\n ...\n \"audit-log-maxbackup\": [10],\n ...",
      "id": "xccdf_org.ssgproject.content_rule_ocp_api_server_audit_log_maxbackup",
      "instructions": "Run the following command:\n$ oc get configmap config -n openshift-apiserver -ojson | jq -r '.data[\"config.yaml\"]' | jq '.apiServerArguments[\"audit-log-maxbackup\"][0]'\nThe output should return a value of 10 or as appropriate.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "ocp-api-server-audit-log-maxbackup"
        },
        "creationTimestamp": "2024-03-05T15:10:44Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "low",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-ocp-api-server-audit-log-maxbackup",
        "namespace": "openshift-compliance",
        "resourceVersion": "82729",
        "uid": "f89a0118-729f-432f-9df1-34a1e2963361"
      },
      "rationale": "OpenShift automatically rotates the log files. Retaining old log files ensures OpenShift Operators will have sufficient log data available for carrying out any investigation or correlation. For example, if the audit log size is set to 100 MB and the number of retained log files is set to 10, OpenShift Operators would have approximately 1 GB of log data to use during analysis.",
      "severity": "low",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Configure OpenShift API Server Maximum Audit Log Size\nTo rotate audit logs upon reaching a maximum size, edit the openshift-apiserver configmap and set the audit-log-maxsize parameter to an appropriate size in MB. For example, to set it to 100 MB:\n\n\"apiServerArguments\":{\n ...\n \"audit-log-maxsize\": [\"100\"],\n ...",
      "id": "xccdf_org.ssgproject.content_rule_ocp_api_server_audit_log_maxsize",
      "instructions": "Run the following command:\n$ oc get configmap config -n openshift-apiserver -ojson | jq -r '.data[\"config.yaml\"]' | jq '.apiServerArguments[\"audit-log-maxsize\"]'\nThe output should return a value of [\"100\"] or as appropriate.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "ocp-api-server-audit-log-maxsize"
        },
        "creationTimestamp": "2024-03-05T15:10:43Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-ocp-api-server-audit-log-maxsize",
        "namespace": "openshift-compliance",
        "resourceVersion": "82702",
        "uid": "6e324921-2698-4b93-9399-ac3617fbbe77"
      },
      "rationale": "OpenShift automatically rotates log files. Retaining old log files ensures that OpenShift Operators have sufficient log data available for carrying out any investigation or correlation. If you have set file size of 100 MB and the number of old log files to keep as 10, there would be approximately 1 GB of log data available for use in analysis.",
      "severity": "medium",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Check configured allowed registries for import uses secure protocol\nThe configuration allowedRegistriesForImport limits the container image registries from which normal users may import images. This is a list of the registries that can be trusted to contain valid images and the image location configured is assumed to be secured unless configured otherwise. It is important to allow only secure registries to avoid man in the middle attacks, as the insecure image import request can be impersonated and could lead to fetching malicious content. List all the allowed repositories for import configured with insecure set to true using the following command:\n\noc get image.config.openshift.io/cluster -o json | jq '.spec | (.allowedRegistriesForImport[])? | select(.insecure==true)'\n\nRemove or edit the listed registries having insecure set by using the command:\n\noc edit image.config.openshift.io/cluster\n\nFor more information, follow the relevant documentation ( https://docs.openshift.com/container-platform/latest/openshift_images/image-configuration.html ).",
      "id": "xccdf_org.ssgproject.content_rule_ocp_insecure_allowed_registries_for_import",
      "instructions": "To check for the configured allowedRegistriesForImport with insecure option use below command:\n$ oc get image.config.openshift.io/cluster -ojsonpath='{.spec.allowedRegistriesForImport}'\nThe output lists the configured allowedRegistriesForImport and the insecure option used.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "ocp-insecure-allowed-registries-for-import"
        },
        "creationTimestamp": "2024-03-05T15:10:50Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-ocp-insecure-allowed-registries-for-import",
        "namespace": "openshift-compliance",
        "resourceVersion": "82819",
        "uid": "da8d85b6-f217-4917-85af-ddef351ee6ae"
      },
      "rationale": "Configured list of allowed registries for import should be from the secure source.",
      "severity": "medium",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Check if any insecure registry sources is configured\nThe configuration registrySources.insecureRegistries determines the insecure registries that the OpenShift container runtime can access for builds and pods. This configuration setting is for accessing the configured registries without TLS validation which could lead to security breaches and should be avoided. Remove any insecureRegistries configured using the following command:\n\noc patch image.config.openshift.io cluster --type=json -p \"[{'op': 'remove', 'path': '/spec/registrySources/insecureRegistries'}]\"\n\nFor more information, follow the relevant documentation ( https://docs.openshift.com/container-platform/latest/openshift_images/image-configuration.html ).",
      "id": "xccdf_org.ssgproject.content_rule_ocp_insecure_registries",
      "instructions": "To check for the configured insecure registry sources use below command:\n$ oc get image.config.openshift.io/cluster -ojsonpath='{.spec.registrySources.insecureRegistries}'\nThe output lists the insecure registry sources configured.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "ocp-insecure-registries"
        },
        "creationTimestamp": "2024-03-05T15:10:49Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-ocp-insecure-registries",
        "namespace": "openshift-compliance",
        "resourceVersion": "82801",
        "uid": "6cfd7959-f1e1-4402-83ec-65ce550fbaa3"
      },
      "rationale": "Insecure registries should not be configured, which would restrict the possibilities of OpenShift container runtime accessing registries which cannot be validated.",
      "severity": "medium",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Configure the Audit Log Path\nTo enable auditing on the OpenShift API Server, the audit log path must be set. Edit the openshift-apiserver configmap and set the audit-log-path to a suitable path and file where audit logs should be written. For example:\n\n\"apiServerArguments\":{\n ...\n \"audit-log-path\":\"/var/log/openshift-apiserver/audit.log\",\n ...",
      "id": "xccdf_org.ssgproject.content_rule_openshift_api_server_audit_log_path",
      "instructions": "Run the following command:\n$ oc get configmap config -n openshift-apiserver -ojson | jq -r '.data[\"config.yaml\"]' | jq '.apiServerArguments[\"audit-log-path\"]'\nThe output should return a valid audit log path.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "openshift-api-server-audit-log-path"
        },
        "creationTimestamp": "2024-03-05T15:10:49Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "high",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-openshift-api-server-audit-log-path",
        "namespace": "openshift-compliance",
        "resourceVersion": "82792",
        "uid": "e727f27f-73f4-4032-ab8d-86eea05652ec"
      },
      "rationale": "Auditing of the API Server is not enabled by default. Auditing the API Server provides a security-relevant chronological set of records documenting the sequence of activities that have affected the system by users, administrators, or other system components.",
      "severity": "high",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Profiling is protected by RBAC\nEnsure that the cluster-debugger cluster role includes the /debug/pprof resource URL. This demonstrates that profiling is protected by RBAC, with a specific cluster role to allow access.",
      "id": "xccdf_org.ssgproject.content_rule_rbac_debug_role_protects_pprof",
      "instructions": "To verify that the cluster-debugger role is configured correctly,\nrun the following command:\n$ oc get clusterroles cluster-debugger -o jsonpath='{.rules[0].nonResourceURLs}'\nand verify that the /debug/pprof path is included there.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "rbac-debug-role-protects-pprof"
        },
        "creationTimestamp": "2024-03-05T15:10:46Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-rbac-debug-role-protects-pprof",
        "namespace": "openshift-compliance",
        "resourceVersion": "82758",
        "uid": "817fe171-5f8d-48df-b9b6-1a8250826e42"
      },
      "rationale": "Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface. To ensure the collected data is not exploited, profiling endpoints are secured via RBAC (see cluster-debugger role). By default, the profiling endpoints are accessible only by users bound to cluster-admin or cluster-debugger role. Profiling can not be disabled.",
      "severity": "medium",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Ensure that the RBAC setup follows the principle of least privilege\nRole-based access control (RBAC) objects determine whether a user is allowed to perform a given action within a project. If users or groups exist that are bound to roles they must not have, modify the user or group permissions using the following cluster and local role binding commands: Remove a User from a Cluster RBAC role by executing the following: oc adm policy remove-cluster-role-from-user role username Remove a Group from a Cluster RBAC role by executing the following: oc adm policy remove-cluster-role-from-group role groupname Remove a User from a Local RBAC role by executing the following: oc adm policy remove-role-from-user role username Remove a Group from a Local RBAC role by executing the following: oc adm policy remove-role-from-group role groupname NOTE: For additional information. https://docs.openshift.com/container-platform/latest/authentication/using-rbac.html",
      "id": "xccdf_org.ssgproject.content_rule_rbac_least_privilege",
      "instructions": "The administrator must verify that Openshift is configured with the necessary RBAC access controls.\n\nReview the RBAC configuration.\n\nAs the cluster-admin, view the cluster roles and their associated rule sets by executing the following::\n\noc describe clusterrole.rbac\n\nNow view the current set of cluster role bindings, which shows the users and groups that are bound to various roles by executing the following:\n\noc describe clusterrolebinding.rbac\n\nLocal roles and bindings can be determined using the follow commands by executing the following:\n\noc describe rolebinding.rbac\n\nIf these results show users with privileged access that do not require that access, this is a finding.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "rbac-least-privilege"
        },
        "creationTimestamp": "2024-03-05T15:10:50Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "high",
          "compliance.openshift.io/check-status": "MANUAL",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-rbac-least-privilege",
        "namespace": "openshift-compliance",
        "resourceVersion": "82816",
        "uid": "7bf517f7-4333-42ed-82de-a6aeb1f77069"
      },
      "rationale": "Controlling and limiting users access to system services and resources is key to securing the platform and limiting the intentional or unintentional comprimising of the system and its services. OpenShift provides a robust RBAC policy system that allows for authorization policies to be as detailed as needed. Additionally there are two layers of RBAC policies, the first is Cluster RBAC policies which administrators can control who has what access to cluster level services. The other is Local RBAC policies, which allow project developers/administrators to control what level of access users have to a given project or namespace.",
      "severity": "high",
      "status": "MANUAL"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Ensure that the cluster-admin role is only used where required\nThe RBAC role cluster-admin provides wide-ranging powers over the environment and should be used only where and when needed.",
      "id": "xccdf_org.ssgproject.content_rule_rbac_limit_cluster_admin",
      "instructions": "Review users and groups bound to cluster-admin and decide whether they\nrequire such access. Consider creating least-privilege roles for users and\nservice accounts. Obtain a list of the users who have access to the\ncluster-admin role by reviewing the clusterrolebinding output for each role\nbinding that has access to the cluster-admin role. To do this, run the\nfollowing command:\n$ oc get clusterrolebindings -o=custom-columns=NAME:.metadata.name,ROLE:.roleRef.name,SUBJECT:.subjects[*].kind | grep cluster-admin\n\nCare should be taken before removing any clusterrolebindings from the\nenvironment to ensure they are not required for operation of the cluster.\nSpecifically, modifications should not be made to the default\nclusterrolebindings including those with the \"system:\" prefix.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "rbac-limit-cluster-admin"
        },
        "creationTimestamp": "2024-03-05T15:10:48Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "MANUAL",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-rbac-limit-cluster-admin",
        "namespace": "openshift-compliance",
        "resourceVersion": "82778",
        "uid": "f7289741-beef-40c6-b19f-3ec8ed2efcc5"
      },
      "rationale": "Kubernetes provides a set of default roles where RBAC is used. Some of these roles such as cluster-admin provide wide-ranging privileges which should only be applied where absolutely necessary. Roles such as cluster-admin allow super-user access to perform any action on any resource. When used in a ClusterRoleBinding, it gives full control over every resource in the cluster and in all namespaces. When used in a RoleBinding, it gives full control over every resource in the rolebinding's namespace, including the namespace itself.",
      "severity": "medium",
      "status": "MANUAL"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Limit Access to Kubernetes Secrets\nThe Kubernetes API stores secrets, which may be service account tokens for the Kubernetes API or credentials used by workloads in the cluster. Access to these secrets should be restricted to the smallest possible group of users to reduce the risk of privilege escalation. To restrict users from secrets, remove get , list , and watch access to unauthorized users to secret objects in the cluster.",
      "id": "xccdf_org.ssgproject.content_rule_rbac_limit_secrets_access",
      "instructions": "To review the policy rules assigned to roles in all namespaces, run\nthe following command:\n$ for ns in $(oc get projects -ojsonpath='{.items[*].metadata.name}'); do oc describe roles -n$ns; done\nTo review the policy rules assigned to cluster roles, run the following\ncommand:\n$ for i in $(oc get clusterroles -o jsonpath='{.items[*].metadata.name}'); do oc describe clusterrole ${i}; done\nReview the output and ensure that only authorized roles have access to the\nsecrets resource or all resources using a wildcard.\nTo filter clusterroles that have assigned access to the secrets resources for clustrroles, run:\n$ oc get clusterroles -ojson | jq -r '.items[] | {name: .metadata.name, rules: .rules} | select(.rules[]?.resources | try contains([\"secrets\"])) | .name' | sort | uniq\nand similarly to filter namespace/role pairs:\n$ for ns in $(oc get projects -ojsonpath='{.items[*].metadata.name}'); do oc get roles -n$ns -ojson | jq -r '.items[] | {name: .metadata.name, namespace: .metadata.namespace, rules: .rules} | select(.rules[]?.resources | try contains([\"secrets\"])) | \"\\(.namespace)/\\(.name)\"' | sort | uniq; done\nnote that the two commands above do not show roles and/or clusterroles with wildcard access to any resources.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "rbac-limit-secrets-access"
        },
        "creationTimestamp": "2024-03-05T15:10:46Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "MANUAL",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-rbac-limit-secrets-access",
        "namespace": "openshift-compliance",
        "resourceVersion": "82759",
        "uid": "41e47a2f-6333-412a-8286-aacda590025b"
      },
      "rationale": "Inappropriate access to secrets stored within the Kubernetes cluster can allow for an attacker to gain additional access to the Kubernetes cluster or external resources whose credentials are stored as secrets.",
      "severity": "medium",
      "status": "MANUAL"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Minimize Access to Pod Creation\nThe ability to create pods in a namespace can provide a number of opportunities for privilege escalation. Where applicable, remove create access to pod objects in the cluster.",
      "id": "xccdf_org.ssgproject.content_rule_rbac_pod_creation_access",
      "instructions": "To review the pod creation privileges in roles, run the following commands:\n$ oc describe roles --all-namespaces\n$ oc describe clusterroles\nReview the output, and for any role/clusterrole defining create permissions\nfor pods that are NOT an OpenShift \"system:\" or other system-provided\nrole/clusterrole, determine if the users bound to the role truly have the\nneed to create pods.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "rbac-pod-creation-access"
        },
        "creationTimestamp": "2024-03-05T15:10:45Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "MANUAL",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-rbac-pod-creation-access",
        "namespace": "openshift-compliance",
        "resourceVersion": "82741",
        "uid": "5d2b9719-ecce-4dec-98f8-8a729b1e8bee"
      },
      "rationale": "The ability to create pods in a cluster opens up the cluster for privilege escalation.",
      "severity": "medium",
      "status": "MANUAL"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Minimize Wildcard Usage in Cluster and Local Roles\nKubernetes Cluster and Local Roles provide access to resources based on sets of objects and actions that can be taken on those objects. It is possible to set either of these using a wildcard * which matches all items. This violates the principle of least privilege and leaves a cluster in a more vulnerable state to privilege abuse.",
      "id": "xccdf_org.ssgproject.content_rule_rbac_wildcard_use",
      "instructions": "To review the wildcard usage in roles, run the following commands:\n$ oc describe roles --all-namespaces\n$ oc describe clusterroles\nReview the output, and for any role/clusterrole specifying a wildcard\nresource that is NOT an OpenShift \"system:\" or other system-provided\nrole/clusterrole, determine if the wildcard access can be replaced with\nspecific resources.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "rbac-wildcard-use"
        },
        "creationTimestamp": "2024-03-05T15:10:50Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "MANUAL",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-rbac-wildcard-use",
        "namespace": "openshift-compliance",
        "resourceVersion": "82818",
        "uid": "3d214726-55db-4174-9d6e-767b942585ea"
      },
      "rationale": "The principle of least privilege recommends that users are provided only the access required for their role and nothing more. The use of wildcard rights grants is likely to provide excessive rights to the Kubernetes API.",
      "severity": "medium",
      "status": "MANUAL"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Drop Container Capabilities\nContainers should not enable more capabilities than needed as this opens the door for malicious use. To disable the capabilities, the appropriate Security Context Constraints (SCCs) should set all capabilities as * or a list of capabilities in requiredDropCapabilities.",
      "id": "xccdf_org.ssgproject.content_rule_scc_drop_container_capabilities",
      "instructions": "Inspect each SCC returned from running the following command:\n$ oc get scc\nNext, examine the outputs of the following commands:\n$ oc describe roles --all-namespaces\n$ oc describe clusterroles\nFor any role/clusterrole that reference the\nsecuritycontextconstraints resource with the resourceNames\nof the SCCs that do not list any requiredDropCapabilities, examine the\nassociated rolebindings to account for the users that are bound to the role.\nReview each SCC and determine that all capabilities are either\ncompletely disabled as a list entry under requiredDropCapabilities,\nor that all the un-required capabilities are dropped for containers and SCCs.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "scc-drop-container-capabilities"
        },
        "creationTimestamp": "2024-03-05T15:10:44Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "MANUAL",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-scc-drop-container-capabilities",
        "namespace": "openshift-compliance",
        "resourceVersion": "82724",
        "uid": "beeb47ed-8240-4015-8e5c-4d9cd9fdb056"
      },
      "rationale": "By default, containers run with a default set of capabilities as assigned by the Container Runtime which can include dangerous or highly privileged capabilities. Capabilities should be dropped unless absolutely critical for the container to run software as added capabilities that are not required allow for malicious containers or attackers.",
      "severity": "medium",
      "status": "MANUAL"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Limit Container Capabilities\nContainers should not enable more capabilites than needed as this opens the door for malicious use. To enable only the required capabilities, the appropriate Security Context Constraints (SCCs) should set capabilities as a list in allowedCapabilities.\n\nIn case an SCC outside the default allow list in the variable var-sccs-with-allowed-capabilities-regex is being flagged, create a TailoredProfile and add the additional SCC to the regular expression in the variable var-sccs-with-allowed-capabilities-regex. An example allowing an SCC named additional follows:\n\napiVersion: compliance.openshift.io/v1alpha1\nkind: TailoredProfile\nmetadata:\n name: cis-additional-scc\nspec:\n description: Allows an additional scc\n setValues:\n - name: upstream-ocp4-var-sccs-with-allowed-capabilities-regex\n   rationale: Allow our own custom SCC\n   value: ^privileged$|^hostnetwork-v2$|^restricted-v2$|^nonroot-v2$|^additional$\n extends: upstream-ocp4-cis\n title: Modified CIS allowing one more SCC\n\nFinally, reference this TailoredProfile in a ScanSettingBinding For more information on Tailoring the Compliance Operator, please consult the OpenShift documentation: https://docs.openshift.com/container-platform/4.12/security/compliance_operator/compliance-operator-tailor.html",
      "id": "xccdf_org.ssgproject.content_rule_scc_limit_container_allowed_capabilities",
      "instructions": "This rule checks the SCCs with allowedCapabilities set to non-null\nand fails if there are more such SCCs than those allowed in the variable\nnamed ocp4-var-sccs-with-allowed-capabilities-regex. To debug the rule,\ncheck the variable value, e.g:\n$ oc get variable ocp4-var-sccs-with-allowed-capabilities-regex  -ojsonpath='{.value}' \nThen use following command to list the SCCs that would fail the test:\n$ oc get scc -o json | jq '[.items[] | select(.metadata.name | test(\"^privileged$|^hostnetwork-v2$|^restricted-v2$|^nonroot-v2$\"; \"\") | not) | select(.allowedCapabilities != null) | .metadata.name]'\nPlease replace the regular expression in the test command with the value read from the variable\nocp4-var-sccs-with-allowed-capabilities-regex. You can read the variable\nvalue with:\n$ oc get variable ocp4-var-sccs-with-allowed-capabilities-regex -ojsonpath='{.value}' -n openshift-compliance",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "scc-limit-container-allowed-capabilities"
        },
        "creationTimestamp": "2024-03-05T15:10:51Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-has-value": "",
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-scc-limit-container-allowed-capabilities",
        "namespace": "openshift-compliance",
        "resourceVersion": "82830",
        "uid": "d92b7e3d-9e7e-4b01-8eb8-ed2647ccf9b9"
      },
      "rationale": "By default, containers run with a default set of capabilities as assigned by the Container Runtime which can include dangerous or highly privileged capabilities. Capabilities should be dropped unless absolutely critical for the container to run software as added capabilities that are not required allow for malicious containers or attackers.",
      "severity": "medium",
      "status": "PASS",
      "valuesUsed": [
        "var_sccs_with_allowed_capabilities_regex"
      ]
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Limit Access to the Host IPC Namespace\nContainers should not be allowed access to the host's Interprocess Communication (IPC) namespace. To prevent containers from getting access to a host's IPC namespace, the appropriate Security Context Constraints (SCCs) should set allowHostIPC to false.",
      "id": "xccdf_org.ssgproject.content_rule_scc_limit_ipc_namespace",
      "instructions": "Inspect each SCC returned from running the following command:\n$ oc get scc\nReview each SCC for those that have allowHostIPC set to true.\nNext, examine the outputs of the following commands:\n$ oc describe roles --all-namespaces\n$ oc describe clusterroles\nFor any role/clusterrole that reference the\nsecuritycontextconstraints resource with the resourceNames\nof the SCCs that have allowHostIPC, examine the associated\nrolebindings to account for the users that are bound to the role. Review the\naccount to determine if allowHostIPC is truly required.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "scc-limit-ipc-namespace"
        },
        "creationTimestamp": "2024-03-05T15:10:47Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "MANUAL",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-scc-limit-ipc-namespace",
        "namespace": "openshift-compliance",
        "resourceVersion": "82761",
        "uid": "a2e1e0ea-b099-4d80-96ae-e304724c5dea"
      },
      "rationale": "A container running in the host's IPC namespace can use IPC to interact with processes outside the container potentially allowing an attacker to exploit a host process thereby enabling an attacker to exploit other services.",
      "severity": "medium",
      "status": "MANUAL"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Limit Use of the CAP_NET_RAW\nContainers should not enable more capabilities than needed as this opens the door for malicious use. CAP_NET_RAW enables a container to launch a network attack on another container or cluster. To disable the CAP_NET_RAW capability, the appropriate Security Context Constraints (SCCs) should set NET_RAW in requiredDropCapabilities.",
      "id": "xccdf_org.ssgproject.content_rule_scc_limit_net_raw_capability",
      "instructions": "Inspect each SCC returned from running the following command:\n$ oc get scc\nReview each SCC for those that do not have NET_RAW or ALL set under requiredDropCapabilities.\nNext, examine the outputs of the following commands:\n$ oc describe roles --all-namespaces\n$ oc describe clusterroles\nFor any role/clusterrole that reference the\nsecuritycontextconstraints resource with the resourceNames\nof the SCCs that do not drop NET_RAW or ALL, examine the\nassociated rolebindings to account for the users that are bound to the role.\nReview each SCC and determine that either NET_RAW or ALL\nis either included as a list entry under requiredDropCapabilities,\nor that either NET_RAW or ALL is only enabled to a small\nset of containers and SCCs.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "scc-limit-net-raw-capability"
        },
        "creationTimestamp": "2024-03-05T15:10:49Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "MANUAL",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-scc-limit-net-raw-capability",
        "namespace": "openshift-compliance",
        "resourceVersion": "82804",
        "uid": "dd19b53f-aae4-4826-ab2f-2caab307e127"
      },
      "rationale": "By default, containers run with a default set of capabilities as assigned by the Container Runtime which can include dangerous or highly privileged capabilities. If the CAP_NET_RAW is enabled, it may be misused by malicious containers or attackers.",
      "severity": "medium",
      "status": "MANUAL"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Limit Access to the Host Network Namespace\nContainers should not be allowed access to the host's network namespace. To prevent containers from getting access to a host's network namespace, the appropriate Security Context Constraints (SCCs) should set allowHostNetwork to false.",
      "id": "xccdf_org.ssgproject.content_rule_scc_limit_network_namespace",
      "instructions": "Inspect each SCC returned from running the following command:\n$ oc get scc\nReview each SCC for those that have allowHostNetwork set to true.\nNext, examine the outputs of the following commands:\n$ oc describe roles --all-namespaces\n$ oc describe clusterroles\nFor any role/clusterrole that reference the\nsecuritycontextconstraints resource with the resourceNames\nof the SCCs that have allowHostNetwork, examine the associated\nrolebindings to account for the users that are bound to the role. Review the\naccount to determine if allowHostNetwork is truly required.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "scc-limit-network-namespace"
        },
        "creationTimestamp": "2024-03-05T15:10:49Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "MANUAL",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-scc-limit-network-namespace",
        "namespace": "openshift-compliance",
        "resourceVersion": "82797",
        "uid": "e8ced39a-c229-44a3-b8ea-3a218f597415"
      },
      "rationale": "A container running in the host's network namespace could access the host network traffic to and from other pods potentially allowing an attacker to exploit pods and network traffic.",
      "severity": "medium",
      "status": "MANUAL"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Limit Containers Ability to Escalate Privileges\nContainers should be limited to only the privileges required to run and should not be allowed to escalate their privileges. To prevent containers from escalating privileges, the appropriate Security Context Constraints (SCCs) should set allowPrivilegeEscalation to false.",
      "id": "xccdf_org.ssgproject.content_rule_scc_limit_privilege_escalation",
      "instructions": "Inspect each SCC returned from running the following command:\n$ oc get scc\nReview each SCC for those that have allowPrivilegeEscalation set to true.\nNext, examine the outputs of the following commands:\n$ oc describe roles --all-namespaces\n$ oc describe clusterroles\nFor any role/clusterrole that reference the\nsecuritycontextconstraints resource with the resourceNames\nof the SCCs that have allowPrivilegeEscalation, examine the associated\nrolebindings to account for the users that are bound to the role. Review the\naccount to determine if allowPrivilegeEscalation is truly required.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "scc-limit-privilege-escalation"
        },
        "creationTimestamp": "2024-03-05T15:10:43Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "MANUAL",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-scc-limit-privilege-escalation",
        "namespace": "openshift-compliance",
        "resourceVersion": "82715",
        "uid": "188bb006-9e0c-432e-9421-01b7092473e2"
      },
      "rationale": "Privileged containers have access to more of the Linux Kernel capabilities and devices. If a privileged container were compromised, an attacker would have full access to the container and host.",
      "severity": "medium",
      "status": "MANUAL"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Limit Privileged Container Use\nContainers should be limited to only the privileges required to run. To prevent containers from running as privileged containers, the appropriate Security Context Constraints (SCCs) should set allowPrivilegedContainer to false.",
      "id": "xccdf_org.ssgproject.content_rule_scc_limit_privileged_containers",
      "instructions": "Inspect each SCC and the users and groups allowed to use it returned\nfrom running the following command:\noc get scc -ojson | jq '.items[]|select(.allowPrivilegedContainer)|.metadata.name,{\"Group:\":.groups},{\"User\":.users}'\n\nThe group \"system:authenticated\" is the default group for any\nauthenticated user, this group should only be associated with the\nrestricted profile. If this group is listed under any other SCC Policy,\nor the restricted SCC policy has been altered to allow any of the\nnon-permitted actions, this is a finding.\n\nNext, determine if there are any cluster roles or local roles that allow\nthe use of use of non-permitted SCC policies.  The following commands will\nprint the Role's name and namespace, followed by a list of resource names\nand if that resource is an SCC.\n\n> oc get clusterrole.rbac -ojson | jq -r '.items[]|select(.rules[]?|select( (.apiGroups[]? == (\"security.openshift.io\")) and (.resources[]? == (\"securitycontextconstraints\")) and (.verbs[]? == (\"use\"))))|.metadata.name,{\"scc\":(.rules[]?|select((.resources[]? == (\"securitycontextconstraints\"))).resourceNames[]?)}'\n\n> oc get role.rbac --all-namespaces -ojson | jq -r '.items[]|select(.rules[]?|select( (.apiGroups[]? == (\"security.openshift.io\")) and (.resources[]? == (\"securitycontextconstraints\")) and (.verbs[]? == (\"use\"))))|.metadata.name,{\"scc\":(.rules[]?|select((.resources[]? == (\"securitycontextconstraints\"))).resourceNames[]?)}'\n\nExcluding platform specific roles, identify any roles that allow use of non-permitted SCC policies for example the follow output shows that the role 'examplePrivilegedRole' allows use of the 'privileged' SCC.\n\n\nexamplePrivilegedRole\n{\n  \"scc\": \"privileged\"\n}\n\n\nFinally, determine if there are any role bindings to cluster or local\nroles that allow use of non-permitted SCCs.\n\n> oc get clusterrolebinding.rbac -ojson | jq -r '.items[]|select(.roleRef.kind == (\"ClusterRole\",\"Role\") and .roleRef.name == (\"[CLUSTER_ROLE_LIST]\"))|{ \"crb\": .metadata.name, \"roleRef\": .roleRef, \"subjects\": .subjects}'\n> oc get rolebinding.rbac --all-namespaces -ojson | jq -r '.items[]|select(.roleRef.kind == (\"ClusterRole\",\"Role\") and .roleRef.name == (\"[LOCAL_ROLE_LIST]\"))|{ \"crb\": .metadata.name, \"roleRef\": .roleRef, \"subjects\": .subjects}'\n\nWhere \"[CLUSTER_ROLE_LIST]\" and \"[LOCAL_ROLE_LIST]\" are\ncomma-separated lists of the roles allowing use of non-permitted SCC\npolicies as identified above. For example:\n\n\n... .roleRef.name == (\"system:openshift:scc:privileged\",\"system:openshift:scc:hostnetwork\",\"system:openshift:scc:hostaccess\") ...\n\n\nExcluding any platform namespaces (kube-*,openshift-*), if there are any rolebindings to roles that are not permitted, this is a finding.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "scc-limit-privileged-containers"
        },
        "creationTimestamp": "2024-03-05T15:10:45Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "MANUAL",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-scc-limit-privileged-containers",
        "namespace": "openshift-compliance",
        "resourceVersion": "82742",
        "uid": "3190b8a3-c463-4e96-82a4-dbd8c89e8ca2"
      },
      "rationale": "Privileged containers have access to all Linux Kernel capabilities and devices. If a privileged container were compromised, an attacker would have full access to the container and host.",
      "severity": "medium",
      "status": "MANUAL"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Limit Access to the Host Process ID Namespace\nContainers should not be allowed access to the host's process ID namespace. To prevent containers from getting access to a host's process ID namespace, the appropriate Security Context Constraints (SCCs) should set allowHostPID to false.",
      "id": "xccdf_org.ssgproject.content_rule_scc_limit_process_id_namespace",
      "instructions": "Inspect each SCC returned from running the following command:\n$ oc get scc\nReview each SCC for those that have allowHostPID set to true.\nNext, examine the outputs of the following commands:\n$ oc describe roles --all-namespaces\n$ oc describe clusterroles\nFor any role/clusterrole that reference the\nsecuritycontextconstraints resource with the resourceNames\nof the SCCs that have allowHostPID, examine the associated\nrolebindings to account for the users that are bound to the role. Review the\naccount to determine if allowHostPID is truly required.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "scc-limit-process-id-namespace"
        },
        "creationTimestamp": "2024-03-05T15:10:45Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "MANUAL",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-scc-limit-process-id-namespace",
        "namespace": "openshift-compliance",
        "resourceVersion": "82743",
        "uid": "d32ec465-fd32-40b4-939b-f389fbfcdd58"
      },
      "rationale": "A container running in the host's PID namespace can inspect processes running outside the container which can be used to escalate privileges outside of the container.",
      "severity": "medium",
      "status": "MANUAL"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Limit Container Running As Root User\nContainers should run as a random non-privileged user. To prevent containers from running as root user, the appropriate Security Context Constraints (SCCs) should set.runAsUser.type to MustRunAsRange.",
      "id": "xccdf_org.ssgproject.content_rule_scc_limit_root_containers",
      "instructions": "Inspect each SCC and the users and groups allowed to use it returned\nfrom running the following command:\noc get scc -ojson | jq '.items[]|select(.runAsUser.type != \"MustRunAsRange\" )|.metadata.name,{\"Group:\":.groups},{\"User\":.users}'\n\nThe group \"system:authenticated\" is the default group for any\nauthenticated user, this group should only be associated with the\nrestricted profile. If this group is listed under any other SCC Policy,\nor the restricted SCC policy has been altered to allow any of the\nnon-permitted actions, this is a finding.\n\nNext, determine if there are any cluster roles or local roles that allow\nthe use of use of non-permitted SCC policies.  The following commands will\nprint the Role's name and namespace, followed by a list of resource names\nand if that resource is an SCC.\n\n> oc get clusterrole.rbac -ojson | jq -r '.items[]|select(.rules[]?|select( (.apiGroups[]? == (\"security.openshift.io\")) and (.resources[]? == (\"securitycontextconstraints\")) and (.verbs[]? == (\"use\"))))|.metadata.name,{\"scc\":(.rules[]?|select((.resources[]? == (\"securitycontextconstraints\"))).resourceNames[]?)}'\n\n> oc get role.rbac --all-namespaces -ojson | jq -r '.items[]|select(.rules[]?|select( (.apiGroups[]? == (\"security.openshift.io\")) and (.resources[]? == (\"securitycontextconstraints\")) and (.verbs[]? == (\"use\"))))|.metadata.name,{\"scc\":(.rules[]?|select((.resources[]? == (\"securitycontextconstraints\"))).resourceNames[]?)}'\n\nExcluding platform specific roles, identify any roles that allow use of non-permitted SCC policies for example the follow output shows that the role 'examplePrivilegedRole' allows use of the 'privileged' SCC.\n\n\nexamplePrivilegedRole\n{\n  \"scc\": \"privileged\"\n}\n\n\nFinally, determine if there are any role bindings to cluster or local\nroles that allow use of non-permitted SCCs.\n\n> oc get clusterrolebinding.rbac -ojson | jq -r '.items[]|select(.roleRef.kind == (\"ClusterRole\",\"Role\") and .roleRef.name == (\"[CLUSTER_ROLE_LIST]\"))|{ \"crb\": .metadata.name, \"roleRef\": .roleRef, \"subjects\": .subjects}'\n> oc get rolebinding.rbac --all-namespaces -ojson | jq -r '.items[]|select(.roleRef.kind == (\"ClusterRole\",\"Role\") and .roleRef.name == (\"[LOCAL_ROLE_LIST]\"))|{ \"crb\": .metadata.name, \"roleRef\": .roleRef, \"subjects\": .subjects}'\n\nWhere \"[CLUSTER_ROLE_LIST]\" and \"[LOCAL_ROLE_LIST]\" are\ncomma-separated lists of the roles allowing use of non-permitted SCC\npolicies as identified above. For example:\n\n\n... .roleRef.name == (\"system:openshift:scc:privileged\",\"system:openshift:scc:hostnetwork\",\"system:openshift:scc:hostaccess\") ...\n\n\nExcluding any platform namespaces (kube-*,openshift-*), if there are any rolebindings to roles that are not permitted, this is a finding.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "scc-limit-root-containers"
        },
        "creationTimestamp": "2024-03-05T15:10:43Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "MANUAL",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-scc-limit-root-containers",
        "namespace": "openshift-compliance",
        "resourceVersion": "82711",
        "uid": "60145e1f-c194-4b57-a2f4-963af12957bf"
      },
      "rationale": "It is strongly recommended that containers running on OpenShift should support running as any arbitrary UID. OpenShift will then assign a random, non-privileged UID to the running container instance. This avoids the risk from containers running with specific uids that could map to host service accounts, or an even greater risk of running as root level service. OpenShift uses the default security context constraints (SCC), restricted, to prevent containers from running as root or other privileged user ids. Pods may be configured to use an scc policy that allows the container to run as a specific uid, including root(0) when approved. Only a cluster administrator may grant the change of an scc policy.",
      "severity": "medium",
      "status": "MANUAL"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Verify that the scheduler API service is protected by RBAC\nDo not bind the scheduler service to non-loopback insecure addresses.",
      "id": "xccdf_org.ssgproject.content_rule_scheduler_profiling_protected_by_rbac",
      "instructions": "In OpenShift 4, The Kubernetes Scheduler operator manages and updates the Kubernetes Scheduler deployed on top of OpenShift. By default, the operator exposes metrics via metrics service. The metrics are collected from the Kubernetes Scheduler operator. Profiling data is sent to `healthzPort`, the port of the localhost `healthz` endpoint. Changing this value may disrupt components that monitor the kubelet health. The default `healthz` `port` value is `10251`, and the `healthz` `bindAddress` is `127.0.0.1`\n\nTo ensure the collected data is not exploited, profiling endpoints are secured via RBAC (see cluster-debugger role). By default, the profiling endpoints are accessible only by users bound to `cluster-admin` or `cluster-debugger` role. Profiling can not be disabled.\n\nThe bind-address argument is not used. Both authentication and authorization are in place.\n\nRun the following command to verify the schedule endpoints:\n\n```\noc -n openshift-kube-scheduler describe endpoints\n```\n\nVerify the `bind-address` and `port` arguments are not used:\n\n```\noc -n openshift-kube-scheduler get cm kube-scheduler-pod -o json | jq -r '.data.\"pod.yaml\"' | jq '.spec.containers[]|select(.name==\"kube-scheduler\")|.args'\n```\n\nVerify the metrics endpoint is protected by RBAC.\n\nFirst, find the schedule pod information:\n\n```\noc project openshift-kube-scheduler\nexport POD=$(oc get pods -l app=openshift-kube-scheduler -o jsonpath='{.items[0].metadata.name}')\nexport POD_IP=$(oc get pods -l app=openshift-kube-scheduler -o jsonpath='{.items[0].status.podIP}')\nexport PORT=$(oc get pod $POD -o jsonpath='{.spec.containers[0].livenessProbe.httpGet.port}')\n```\n\nAttempt to make an insecure `GET` request to the metrics endpoint:\n\n```\noc rsh $POD curl https://$POD_IP:$PORT/metrics -k\n```\n\nEnsure an `HTTP 403` is returned.\n\nCreate a test service account:\n\n```\noc create sa permission-test-sa\n```\n\nGenerate a service account token and attempt to access the metrics endpoint:\n\n```\nexport SA_TOKEN=$(oc create token permission-test-sa)\noc rsh $POD curl https://$POD_IP:$PORT/metrics -H \"Authorization: Bearer $SA_TOKEN\" -k\n```\n\nVerify that an `HTTP 403` is returned.\n\nLogin as a cluster administrator and attempt to access the metrics endpoint:\n\n```\nexport CLUSTER_ADMIN_TOKEN=$(oc whoami -t)\noc rsh $POD curl https://$POD_IP:$PORT/metrics -H \"Authorization: Bearer $CLUSTER_ADMIN_TOKEN\" -k\n```\n\nVerify metrics output is returned. Unset environment variables used in the test and delete the test service account:\n\n```\nunset CLUSTER_ADMIN_TOKEN POD PORT SA_TOKEN POD_IP\noc delete sa permission-test-sa\n```",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "scheduler-profiling-protected-by-rbac"
        },
        "creationTimestamp": "2024-03-05T15:10:45Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-scheduler-profiling-protected-by-rbac",
        "namespace": "openshift-compliance",
        "resourceVersion": "82746",
        "uid": "2bd832da-3f5b-4c16-82e1-af6e255b78f3"
      },
      "rationale": "The Scheduler API service which runs on port 10251/TCP by default is used for health and metrics information and is available without authentication or encryption. As such it should only be bound to a localhost interface, to minimize the cluster's attack surface",
      "severity": "medium",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Verify that the scheduler API service is protected by RBAC\nDo not bind the scheduler service to non-loopback insecure addresses.",
      "id": "xccdf_org.ssgproject.content_rule_scheduler_service_protected_by_rbac",
      "instructions": "In OpenShift 4, The Kubernetes Scheduler operator manages and updates the Kubernetes Scheduler deployed on top of OpenShift. By default, the operator exposes metrics via metrics service. The metrics are collected from the Kubernetes Scheduler operator. Profiling data is sent to `healthzPort`, the port of the localhost `healthz` endpoint. Changing this value may disrupt components that monitor the kubelet health. The default `healthz` `port` value is `10251`, and the `healthz` `bindAddress` is `127.0.0.1`\n\nTo ensure the collected data is not exploited, profiling endpoints are secured via RBAC (see cluster-debugger role). By default, the profiling endpoints are accessible only by users bound to `cluster-admin` or `cluster-debugger` role. Profiling can not be disabled.\n\nThe bind-address argument is not used. Both authentication and authorization are in place.\n\nRun the following command to verify the schedule endpoints:\n\n```\noc -n openshift-kube-scheduler describe endpoints\n```\n\nVerify the `bind-address` and `port` arguments are not used:\n\n```\noc -n openshift-kube-scheduler get cm kube-scheduler-pod -o json | jq -r '.data.\"pod.yaml\"' | jq '.spec.containers[]|select(.name==\"kube-scheduler\")|.args'\n```\n\nVerify the metrics endpoint is protected by RBAC.\n\nFirst, find the schedule pod information:\n\n```\noc project openshift-kube-scheduler\nexport POD=$(oc get pods -l app=openshift-kube-scheduler -o jsonpath='{.items[0].metadata.name}')\nexport POD_IP=$(oc get pods -l app=openshift-kube-scheduler -o jsonpath='{.items[0].status.podIP}')\nexport PORT=$(oc get pod $POD -o jsonpath='{.spec.containers[0].livenessProbe.httpGet.port}')\n```\n\nAttempt to make an insecure `GET` request to the metrics endpoint:\n\n```\noc rsh $POD curl https://$POD_IP:$PORT/metrics -k\n```\n\nEnsure an `HTTP 403` is returned.\n\nCreate a test service account:\n\n```\noc create sa permission-test-sa\n```\n\nGenerate a service account token and attempt to access the metrics endpoint:\n\n```\nexport SA_TOKEN=$(oc create token permission-test-sa)\noc rsh $POD curl https://$POD_IP:$PORT/metrics -H \"Authorization: Bearer $SA_TOKEN\" -k\n```\n\nVerify that an `HTTP 403` is returned.\n\nLogin as a cluster administrator and attempt to access the metrics endpoint:\n\n```\nexport CLUSTER_ADMIN_TOKEN=$(oc whoami -t)\noc rsh $POD curl https://$POD_IP:$PORT/metrics -H \"Authorization: Bearer $CLUSTER_ADMIN_TOKEN\" -k\n```\n\nVerify metrics output is returned. Unset environment variables used in the test and delete the test service account:\n\n```\nunset CLUSTER_ADMIN_TOKEN POD PORT SA_TOKEN POD_IP\noc delete sa permission-test-sa\n```",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "scheduler-service-protected-by-rbac"
        },
        "creationTimestamp": "2024-03-05T15:10:46Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "PASS",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-scheduler-service-protected-by-rbac",
        "namespace": "openshift-compliance",
        "resourceVersion": "82760",
        "uid": "62a44865-a17a-4b90-a385-ca443da5eefa"
      },
      "rationale": "The Scheduler API service which runs on port 10251/TCP by default is used for health and metrics information and is available without authentication or encryption. As such it should only be bound to a localhost interface, to minimize the cluster's attack surface",
      "severity": "medium",
      "status": "PASS"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Consider external secret storage\nConsider the use of an external secrets storage and management system, instead of using Kubernetes Secrets directly, if you have more complex secret management needs. Ensure the solution requires authentication to access secrets, has auditing of access to and use of secrets, and encrypts secrets. Some solutions also make it easier to rotate secrets.",
      "id": "xccdf_org.ssgproject.content_rule_secrets_consider_external_storage",
      "instructions": "Review the cluster configuration and determine if an appropriate secrets\nmanager has been configured.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "secrets-consider-external-storage"
        },
        "creationTimestamp": "2024-03-05T15:10:44Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "MANUAL",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-secrets-consider-external-storage",
        "namespace": "openshift-compliance",
        "resourceVersion": "82726",
        "uid": "3437949a-1d45-4d73-9225-f5073ef4f664"
      },
      "rationale": "Kubernetes supports secrets as first-class objects, but care needs to be taken to ensure that access to secrets is carefully limited. Using an external secrets provider can ease the management of access to secrets, especially where secrets are used across both Kubernetes and non-Kubernetes environments.",
      "severity": "medium",
      "status": "MANUAL"
    },
    {
      "apiVersion": "compliance.openshift.io/v1alpha1",
      "description": "Do Not Use Environment Variables with Secrets\nSecrets should be mounted as data volumes instead of environment variables.",
      "id": "xccdf_org.ssgproject.content_rule_secrets_no_environment_variables",
      "instructions": "To find workloads that use environment variables for secrets, run the following:\n$ oc get all -o jsonpath='{range .items[?(@..secretKeyRef)]} {.kind} {.metadata.namespace} {.metadata.name} {\"\\n\"}{end}' -A\nReview the output and ensure that workloads that can mount secrets as data\nvolumes use that instead of environment variables.",
      "kind": "ComplianceCheckResult",
      "metadata": {
        "annotations": {
          "compliance.openshift.io/rule": "secrets-no-environment-variables"
        },
        "creationTimestamp": "2024-03-05T15:10:48Z",
        "generation": 1,
        "labels": {
          "compliance.openshift.io/check-severity": "medium",
          "compliance.openshift.io/check-status": "MANUAL",
          "compliance.openshift.io/scan-name": "ocp4-cis",
          "compliance.openshift.io/suite": "compliance-scan-schedule"
        },
        "name": "ocp4-cis-secrets-no-environment-variables",
        "namespace": "openshift-compliance",
        "resourceVersion": "82785",
        "uid": "6cb33446-d45e-443b-880e-14a937391740"
      },
      "rationale": "Environment variables are subject and very susceptible to malicious hijacking methods by an adversary, as such, environment variables should never be used for secrets.",
      "severity": "medium",
      "status": "MANUAL"
    }
  ],
  "kind": "List",
  "metadata": {
    "resourceVersion": ""
  }
}
